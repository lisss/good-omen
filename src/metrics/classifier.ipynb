{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import stanza\n",
    "import requests\n",
    "from html.parser import HTMLParser\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/articles/test/zelen.json') as f:\n",
    "    test_data = json.load(f)\n",
    "with open('../../data/articles/train/train_it_2.json') as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('uk', processors='tokenize,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=100, solver='sag', n_jobs=-1))])\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spo(search_obj):\n",
    "    raw_text, spo = [(k, search_obj[k]) for k in search_obj][0]\n",
    "    subj, pred, obj = [x.lower() if x else x for x in (\n",
    "        spo if spo else [None, None, None])]\n",
    "    return raw_text, subj, pred, obj\n",
    "\n",
    "\n",
    "def search_by_token(token, article):\n",
    "    _, t_subj, _, t_obj = get_spo(article['title'][0])\n",
    "    is_found = get_is_match(token, [t_subj, t_obj])\n",
    "    if not is_found:\n",
    "        for sent in article['content']:\n",
    "            _, s_subj, _, s_obj = get_spo(sent)\n",
    "\n",
    "            is_found = get_is_match(token, [s_subj, s_obj])\n",
    "            if is_found:\n",
    "                break\n",
    "    return is_found\n",
    "\n",
    "\n",
    "def search_relevant_articles(search_term, corpus):\n",
    "    res = []\n",
    "    search_tokens = nlp(search_term).sentences[0].words\n",
    "\n",
    "    for article in corpus:\n",
    "        is_found = None\n",
    "        title_obj = article['title'][0]\n",
    "        title, t_subj, _, t_obj = get_spo(article['title'][0])\n",
    "\n",
    "        if len(search_tokens) == 1:\n",
    "            is_found = search_by_token(search_tokens[0], article)\n",
    "        else:\n",
    "            for token in search_tokens:\n",
    "                is_found = search_by_token(token, article)\n",
    "        if is_found:\n",
    "            res.append(\n",
    "                {'url': article['url'], 'date': article['date'], 'title': title})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(corpus):\n",
    "    features, labels = [], []\n",
    "    \n",
    "    for article in corpus:\n",
    "        feat = {}\n",
    "        title_obj = article['title'][0]\n",
    "        title, t_subj, _, t_obj = get_spo(title_obj)\n",
    "        feat['title'] = title\n",
    "        # STOP POINT\n",
    "        # TODO:\n",
    "        # 1) separate feature extractors\n",
    "        # 2) check Mariana's suggestions in PR\n",
    "        # 3) scrape more data\n",
    "        # 4) annotate more data\n",
    "        feat['t_subj'] = t_subj or 'NONE'\n",
    "        features.append(feat)\n",
    "        labels.append(article['relevant'])\n",
    "        \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subj_pred_obj_text(text):\n",
    "    res = []\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sentences:\n",
    "            pred = None\n",
    "            subj = None\n",
    "            obj = None\n",
    "            pred = next(((word.id, word.lemma)\n",
    "                         for word in sent.words if word.deprel == 'root' and word.upos == 'VERB'),\n",
    "                        None)\n",
    "            if pred:\n",
    "\n",
    "                subj = next(((word.id, word.lemma) for word in sent.words if word.deprel ==\n",
    "                             'nsubj' and word.head == int(pred[0])), None)\n",
    "                obj = next(((word.id, word.lemma) for word in sent.words if word.deprel ==\n",
    "                            'obj' and word.head == int(pred[0])), None)\n",
    "            if pred:\n",
    "                res.append(\n",
    "                    {sent.text: (subj[1] if subj else None, pred[1], obj[1] if obj else None)})\n",
    "            else:\n",
    "                res.append({sent.text: None})\n",
    "    except:\n",
    "        print('Failed to create nlp from text that starts with: ' + text[:50])\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_subj_pred_obj(corpus, res_file):\n",
    "    current_content = []\n",
    "    with open(res_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(current_content, f)\n",
    "\n",
    "    for i, art in enumerate(corpus):\n",
    "        title_with_spo = get_subj_pred_obj_text(art['title'])\n",
    "        content_with_spo = get_subj_pred_obj_text(art['content'])\n",
    "        res = {\n",
    "            'url': art['url'],\n",
    "            'date': art['date'],\n",
    "            'title': title_with_spo,\n",
    "            'content': content_with_spo,\n",
    "            'relevant': art['relevant']\n",
    "        }\n",
    "        with open(res_file, 'r', encoding='utf-8') as f:\n",
    "            current_content = json.load(f)\n",
    "        current_content.append(res)\n",
    "\n",
    "        with open(res_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(current_content, f, ensure_ascii=False)\n",
    "        print('>>>', i)\n",
    "    return current_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_subj_pred_obj(test_data, '../../data/articles/train/train_it_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Опублікований фільм Рік президента Зеленського'}, {'title': 'У Зеленського \"не дійшли руки\" до всіх обіцянок'}, {'title': 'Зеленський анонсував скорочення правоохоронних органів'}]\n",
      "[True, False, False]\n"
     ]
    }
   ],
   "source": [
    "print(X[:3])\n",
    "print(y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('dict_vect',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=True)),\n",
       "                ('lrc',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='multinomial', n_jobs=-1,\n",
       "                                    penalty='l2', random_state=42, solver='sag',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.79      0.73      0.76        15\n",
      "        True       0.82      0.86      0.84        21\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.80      0.80      0.80        36\n",
      "weighted avg       0.80      0.81      0.80        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_test, clf.predict(data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
