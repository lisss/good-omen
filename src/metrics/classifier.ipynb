{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import dateparser\n",
    "import datetime\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from eli5.sklearn.utils import get_feature_names\n",
    "import pickle\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import stanza\n",
    "from tokenize_uk import tokenize_uk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 19:36:22 INFO: Loading these models for language: uk (Ukrainian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | iu      |\n",
      "| pos       | iu      |\n",
      "| lemma     | iu      |\n",
      "| depparse  | iu      |\n",
      "=======================\n",
      "\n",
      "2020-06-09 19:36:22 INFO: Use device: cpu\n",
      "2020-06-09 19:36:22 INFO: Loading: tokenize\n",
      "2020-06-09 19:36:22 INFO: Loading: pos\n",
      "2020-06-09 19:36:23 INFO: Loading: lemma\n",
      "2020-06-09 19:36:23 INFO: Loading: depparse\n",
      "2020-06-09 19:36:24 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('uk', processors='tokenize,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=800, solver='sag', n_jobs=-1))])\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotattion_sources(source_path, target_path):\n",
    "    data_files = os.listdir(source_articles_path)\n",
    "    file_count = 0\n",
    "    for file in data_files:\n",
    "        with open(os.path.join(source_path, file)) as f:\n",
    "            art_count = 0\n",
    "            cont = json.load(f)\n",
    "            for i, article in enumerate(cont):\n",
    "                res = []\n",
    "                res.append(article['url'] + '\\n')\n",
    "                res.append('===\\n')\n",
    "                res.append(f'{article[\"title\"]}\\t|\\tREL_{art_count}\\n')\n",
    "                res.append('===\\n')\n",
    "\n",
    "                sents = tokenize_uk.tokenize_sents(article['content'])\n",
    "                for i, sent in enumerate(sents):\n",
    "                    res.append(f'{sent}\\t|\\tREL_{art_count}_{i}\\n')\n",
    "                res.append(f'\\nREL_{art_count}\\n')\n",
    "                res.append('END\\n\\n')\n",
    "                \n",
    "                with open(os.path.join(target_path, f'to_annotate_{file_count}.txt'), 'a') as f:\n",
    "                    f.writelines(res)\n",
    "                    \n",
    "                    if art_count == 99:\n",
    "                        file_count += 1\n",
    "                        art_count = 0\n",
    "                    else:\n",
    "                        art_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "''' Parse annotated data\n",
    "\n",
    "algorythm of parsing annotated data\n",
    "1) if all the article marked as IS_NO_EVENT and there are no other marks inside -\n",
    "mark every sentense as NO_EVENT\n",
    "2) if a block contains more than one sentence - split into multiple sentences\n",
    "by annotated split object (DATE annotation at the sentence beginning)\n",
    "'''\n",
    "\n",
    "\n",
    "def parse_annotated_articles(annotated_articles_path):\n",
    "    files = os.listdir(annotated_articles_path)\n",
    "    lines = []\n",
    "    raw_lines = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(annotated_articles_path, file)) as f:\n",
    "            reader = csv.reader(f, dialect='excel-tab')\n",
    "            count = 0\n",
    "            miltiline_events_cache = []\n",
    "            is_all_non_event = None\n",
    "            all_non_events = []\n",
    "            start_mark = None\n",
    "\n",
    "            for i, row in enumerate(reader):\n",
    "                if row:\n",
    "                    first_column = row[0]\n",
    "                    next_column = row[1]\n",
    "\n",
    "                    if first_column != '===':\n",
    "\n",
    "                        if next_column == 'O':\n",
    "                            fst_col_split = first_column.split(' | ')\n",
    "\n",
    "                            # The line was required to be annotated annotation\n",
    "                            if len(fst_col_split) > 1:\n",
    "                                # The line was annotated, annotation is set on the next line\n",
    "                                if not fst_col_split[1]:\n",
    "                                    mark_row = reader.__next__()\n",
    "                                    is_event = mark_row[1] == 'IS_EVENT'\n",
    "                                    if not start_mark:\n",
    "                                        start_mark = mark_row[0]\n",
    "                                    if is_all_non_event is not None:\n",
    "                                        is_all_non_event = not is_event\n",
    "                                    line = (fst_col_split[0], is_event)\n",
    "                                    if not len(miltiline_events_cache):\n",
    "                                        lines.append(line)\n",
    "                                        if is_all_non_event:\n",
    "                                            all_non_events.append(line)\n",
    "                                    else:\n",
    "                                        miltiline_events_cache.append(line)\n",
    "                                        is_all_non_event = False\n",
    "                                # Not annotated\n",
    "                                else:\n",
    "                                    if is_all_non_event:\n",
    "                                        all_non_events.append(\n",
    "                                            (fst_col_split[0], False))\n",
    "                                    mark = re.findall(\n",
    "                                        'REL_\\\\d+', fst_col_split[1])\n",
    "                                    if mark:\n",
    "                                        start_mark = mark[0]\n",
    "                                if len(miltiline_events_cache):\n",
    "                                    # We already have some sentences in the cache\n",
    "                                    # Updating their status\n",
    "                                    ee = [miltiline_events_cache[i:i + 2]\n",
    "                                          for i in range(0, len(miltiline_events_cache), 2)]\n",
    "                                    for e in ee:\n",
    "                                        r = []\n",
    "                                        for x in e:\n",
    "                                            a, b = x\n",
    "                                            if a not in r:\n",
    "                                                r.append(a.strip())\n",
    "                                        t = ' '.join(r)\n",
    "                                        lines.append((t, is_event))\n",
    "                                    miltiline_events_cache = []\n",
    "                            else:\n",
    "                                event_text = fst_col_split[0].strip()\n",
    "                                # Append multi-event text if we already have something in the cache\n",
    "                                if event_text and len(miltiline_events_cache):\n",
    "                                    miltiline_events_cache.append(\n",
    "                                        (event_text, next_column))\n",
    "                                    is_all_non_event = False\n",
    "                        elif not (row[0].startswith('REL_') or row == 'END'):\n",
    "                            line = (row[0], next_column)\n",
    "                            miltiline_events_cache.append(line)\n",
    "                            is_all_non_event = False\n",
    "                    # We reached the end\n",
    "                    if first_column == start_mark:\n",
    "                        if is_all_non_event and next_column == 'IS_NOT_EVENT':\n",
    "                            lines += all_non_events\n",
    "                        all_non_events = []\n",
    "                count += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(doc):\n",
    "    print(*[f'id: {word.id}\\tword: {word.text}\\tPOS:{word.upos}\\t\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "\n",
    "\n",
    "def get_all_docs(annotated_texts):\n",
    "    docs = []\n",
    "    for i, (text, is_event) in enumerate(annotated_texts):\n",
    "        try:\n",
    "            doc = nlp(text)\n",
    "            docs.append((doc, is_event))\n",
    "        except Exception as e:\n",
    "            print(f'Failed to create nlp from text that starts with: {text[:50]} {e}')\n",
    "        if i % 500 == 0:\n",
    "            print('-->', i)\n",
    "    return docs\n",
    "        \n",
    "\n",
    "def parse_features(feats_string):\n",
    "    res = {}\n",
    "    feats = feats_string.split('|')\n",
    "    for feat in feats:\n",
    "        k, v = feat.split('=')\n",
    "        res[k] = v\n",
    "    return res\n",
    "\n",
    "\n",
    "def find_dates(string, is_future=False):\n",
    "    valid_months = ['січня', 'січ.','лютого','лют.','березня','берез.','квітня','квіт.',\n",
    "                'травня','трав.','червня','черв.','липня','лип.','серпня','серп.',\n",
    "               'вересня','верес.','жовтня','жовт.','листопада','листоп.','грудня','груд.']\n",
    "    mon_regex_str = '|'.join(valid_months).replace('.', '\\.')\n",
    "    regex = '(\\s\\d{4}\\s|(\\d+ (' + mon_regex_str + ')(\\s\\d{4})?)|\\d{2,4}-\\d{2}-\\d{2,4}|\\d{2}.\\d{2}.\\d{2,4}|\\d{2}\\/\\d{2}\\/\\d{2,4})'\n",
    "    matches = re.findall(regex, string, re.IGNORECASE)\n",
    "    dates = []\n",
    "    for match in matches:\n",
    "        date = match[0].strip()\n",
    "        if len(date) == 4:\n",
    "            curr_year = datetime.datetime.now().year\n",
    "            # fragile thing, it may predict date if it's actually some other 4-digit stuff\n",
    "            if int(date) <= curr_year or is_future:\n",
    "                dates.append(date)\n",
    "        else:\n",
    "            dates.append(date)\n",
    "    return dates\n",
    "\n",
    "\n",
    "def get_doc_core_members(doc):\n",
    "    res = []\n",
    "    \n",
    "    adv_final = ['вперше', 'нарешті', 'врешті', 'вчора', 'сьогодні', 'позавчора']\n",
    "    \n",
    "    \n",
    "    def get_token_children(token, tree):\n",
    "        return [x for x in tree if x.head == int(token.id)]\n",
    "    \n",
    "\n",
    "    def get_root_ccomp_verb(root_id, tree):\n",
    "        for word in tree.words:\n",
    "            if word.deprel == 'ccomp' and word.head == root.id:\n",
    "                if word.upos == 'VERB':\n",
    "                    return word\n",
    "                for child in get_token_children(word, tree.words):\n",
    "                    if child.upos == 'VERB':\n",
    "                        return child\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        spo = {}\n",
    "        pred = None\n",
    "        subj = None\n",
    "        obj = None\n",
    "        \n",
    "        num_words = len(sent.words)\n",
    "        \n",
    "        root = next((word\n",
    "                     for word in sent.words if word.deprel == 'root'),\n",
    "                    None)\n",
    "        if not doc.text.strip() or num_words == 2 and sent.words[num_words - 1].upos == 'PUNCT':\n",
    "            continue\n",
    "        # FIXME: iterate only once\n",
    "        if root:\n",
    "            root_conj = next((word for word in sent.words if word.deprel ==\n",
    "                              'conj' and word.head == int(root.id)), None)\n",
    "            root_mod = next((word for word in sent.words if word.deprel ==\n",
    "                              'advmod' and word.upos == 'PART' and word.head == int(root.id)), None)\n",
    "\n",
    "            subj = next((word for word in sent.words if word.deprel ==\n",
    "                         'nsubj' and word.head == int(root.id)), None)\n",
    "            obj = next((word for word in sent.words if word.deprel ==\n",
    "                        'obj' and word.head == int(root_conj.id if root_conj else root.id)),\n",
    "                       None)\n",
    "            c_conj = next((word\n",
    "                     for word in sent.words if word.upos == 'CCONJ' and sent.words[int(word.id) - 2].upos == 'PUNCT'),\n",
    "                    None)\n",
    "            root_adv_final = next((word for word in sent.words if word.deprel ==\n",
    "                        'advmod' and word.upos == 'ADV' and word.head == int(root.id) \\\n",
    "                and word.lemma in adv_final),\n",
    "                       None)\n",
    "            root_xcomp = next((word for word in sent.words if word.deprel ==\n",
    "                              'xcomp' and word.head == int(root.id)), None)\n",
    "            root_ccomp = get_root_ccomp_verb(int(root.id), sent)\n",
    "            root_xcomp_noun = next((word for word in sent.words if word.deprel == 'xcomp:sp' \\\n",
    "                               and word.upos == 'NOUN' \\\n",
    "                              and word.head == int(root.id)),\n",
    "                                    None)\n",
    "\n",
    "            spo['subj'] = subj\n",
    "            spo['root'] = root\n",
    "            spo['root-conj'] = root_conj\n",
    "            spo['obj'] = obj\n",
    "            spo['root_mod'] = root_mod\n",
    "            spo['c_conj'] = c_conj\n",
    "            spo['root_adv_final'] = root_adv_final\n",
    "            spo['root_xcomp'] = root_xcomp\n",
    "            spo['root_ccomp'] = root_ccomp\n",
    "            spo['root_xcomp_noun'] = root_xcomp_noun\n",
    "            if subj:\n",
    "                subj_conj = next((word for word in sent.words if word.deprel ==\n",
    "                             'conj' and (word.upos == 'NOUN' or word.upos == 'PRON') \\\n",
    "                                  and word.head == int(subj.id)), None)\n",
    "                spo['subj-conj'] = subj_conj\n",
    "                if subj_conj:\n",
    "                    subj_conj_verb = next((word for word in sent.words if word.upos ==\n",
    "                        'VERB' and word.head == int(subj_conj.id)),\n",
    "                       None)\n",
    "                    spo['subj-conj-verb'] = subj_conj_verb\n",
    "\n",
    "        res.append((sent.text, spo, num_words))\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def get_features(doc):\n",
    "    features = []\n",
    "    \n",
    "    predicate_special = ['допустити', 'думати', 'припустити', 'відреагувати', 'пояснити',\n",
    "                     'сказати', 'заявити', 'повідомити', 'повідомляти', 'розповісти',\n",
    "                      'розповідати', 'рекомендувати', 'порекомендувати', 'мати', 'стати', 'почати']\n",
    "\n",
    "    spos = get_doc_core_members(doc)\n",
    "#     print(spos)\n",
    "    for sent_text, spo, num_words in spos:\n",
    "        feat = {}\n",
    "        if spo:\n",
    "            root = spo['root']\n",
    "            root_conj = spo.get('root-conj')\n",
    "            root_adv_final = spo.get('root_adv_final')\n",
    "            root_xcomp = spo.get('root_xcomp')\n",
    "            root_ccomp = spo.get('root_ccomp')\n",
    "            root_xcomp_noun = spo.get('root_xcomp_noun')\n",
    "            subj = spo.get('subj')\n",
    "            subj_conj = spo.get('subj-conj')\n",
    "            obj = spo.get('obj')\n",
    "\n",
    "            dates = find_dates(doc.text, True)\n",
    "\n",
    "            if root.feats:\n",
    "                pred_features = parse_features(root.feats)\n",
    "            else:\n",
    "                pred_features = {}\n",
    "\n",
    "            pos_shape = root.upos\n",
    "            if subj:\n",
    "                pos_shape += f'_{subj.upos}'\n",
    "            if obj:\n",
    "                pos_shape += f'_{obj.upos}'\n",
    "            \n",
    "            feat['num-words'] = num_words < 13\n",
    "            feat['pos-shape'] = pos_shape\n",
    "            feat['subj'] = 'SUBJ' if subj else 'NONE'\n",
    "            feat['has-date'] = len(dates) > 0\n",
    "\n",
    "            if pred_features.get('Tense') == 'Past':\n",
    "                feat['root_xcomp'] = root_xcomp is not None\n",
    "                if root_xcomp:\n",
    "                    feat['root_xcomp_pos'] = root_xcomp.upos\n",
    "                if root_ccomp:\n",
    "                    root_ccomp_features = parse_features(root_ccomp.feats)\n",
    "                    feat['root_ccomp_tense'] = root_ccomp_features.get('Tense') or 'NONE'\n",
    "                    feat['root_ccomp_aspect'] = root_ccomp_features.get('Aspect') or 'NONE'\n",
    "                    if root_ccomp_features.get('Tense') != 'Past':\n",
    "                        feat['pred-special'] = root.lemma in predicate_special\n",
    "\n",
    "            if subj:\n",
    "                subj_features = parse_features(subj.feats)\n",
    "                feat['subj-animacy'] = subj_features.get('Animacy') or 'NONE'\n",
    "                feat['subj-pos'] = subj.upos\n",
    "            else:\n",
    "                feat['subj-animacy'] = 'NONE'\n",
    "                feat['subj-pos'] = 'NONE'\n",
    "\n",
    "            feat['obj'] = 'OBJ' if obj else 'NONE'\n",
    "\n",
    "            if root.upos == 'VERB':\n",
    "                feat['pred-tense'] = pred_features.get('Tense') or 'NONE'\n",
    "                feat['pred-aspect'] = pred_features.get('Aspect') or 'NONE'\n",
    "            if root.upos == 'NOUN' or root.upos == 'PROPN':\n",
    "                feat['pred-anim'] = pred_features.get('Animacy') or 'NONE'\n",
    "                feat['pred-abbr'] = pred_features.get('Abbr') or 'NONE'\n",
    "            \n",
    "            features.append(feat)\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def get_data(docs):\n",
    "    def _get_spo_shape(s, p, o):\n",
    "        ids = [p.id]\n",
    "        if s:\n",
    "            ids.append(s.id)\n",
    "        if o:\n",
    "            ids.append(o.id)\n",
    "        indexes = [str(y) for x, y in sorted([(x, i) for i, x in enumerate(ids)])]\n",
    "        \n",
    "        return '_'.join(indexes)\n",
    "\n",
    "    features, labels = [], []\n",
    "\n",
    "    for doc, is_event in docs:\n",
    "        feats = get_features(doc)\n",
    "        for feat in feats:\n",
    "            features.append(feat)\n",
    "            labels.append(is_event if feat else False)\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_articles_path = '../../data/articles/source_normalized'\n",
    "# annotation_source_path = '../../data/articles/for_annotation'\n",
    "# make_annotattion_sources(source_articles_path, annotation_source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: 3189\n",
      "Is event: 1767\n",
      "Is not event: 1422\n"
     ]
    }
   ],
   "source": [
    "annotated_articles_path = '../../data/articles/annotated'\n",
    "annotated_parsed = parse_annotated_articles(annotated_articles_path)\n",
    "annotated_parsed = list(set(annotated_parsed))\n",
    "truish = [(x, y) for x, y in annotated_parsed if y]\n",
    "falsish = [(x, y) for x, y in annotated_parsed if not y]\n",
    "print('All:', len(annotated_parsed))\n",
    "print('Is event:', len(truish))\n",
    "print('Is not event:', len(falsish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../data/articles/annotated_parsed.json', 'w') as f:\n",
    "#     json.dump(annotated_parsed, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> 0\n",
      "--> 500\n",
      "Failed to create nlp from text that starts with: Батькам нікуди дітей дівати. Кернес не ввів карант \n",
      "--> 1000\n",
      "--> 1500\n",
      "--> 2000\n",
      "--> 2500\n",
      "--> 3000\n"
     ]
    }
   ],
   "source": [
    "all_docs = get_all_docs(annotated_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=True\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.029\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=SYM\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.69%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.515\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pred-tense=Past\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.16%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.429\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        has-date\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.53%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.408\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=ADJ_PRON\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.54%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.408\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=VERB_PROPN_PROPN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.62%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.403\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=VERB_PRON_PRON\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.63%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.403\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=ADJ_PROPN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.99%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.383\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        subj-pos=NOUN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.12%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.376\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=ADJ_NOUN_PRON\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.17%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.373\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=VERB_NUM\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.17%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.373\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=X\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.61%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.350\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        subj-pos=PROPN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.66%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.347\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=VERB_PROPN_PRON\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.88%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.335\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=ADJ_X\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 91.36%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.310\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        subj-pos=X\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 91.38%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.309\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=NOUN_NOUN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 91.38%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 25 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 91.14%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 24 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 91.14%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.322\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=VERB_ADJ_NOUN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 90.68%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.346\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=ADJ_ADJ\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 89.98%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.383\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=VERB\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 89.96%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.385\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=ADV_NOUN_PRON\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 89.58%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.406\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        subj-pos=ADJ\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 89.16%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.429\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pred-aspect=Imp\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 89.02%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.437\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=VERB_DET\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.32%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.478\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.89%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.503\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=ADV_NOUN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.30%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.538\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=VERB_NUM_NOUN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.91%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.562\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        subj-pos=PRON\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.27%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.602\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=ADV\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.08%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.678\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=ADJ_NOUN_NOUN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.89%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.965\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pred-tense=Fut\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "Explanation(estimator=\"LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=800,\\n                   multi_class='multinomial', n_jobs=-1, penalty='l2',\\n                   random_state=42, solver='sag', tol=0.0001, verbose=0,\\n                   warm_start=False)\", description=\"\\nFeatures with largest coefficients.\\nCaveats:\\n1. Be careful with features which are not\\n   independent - weights don't show their importance.\\n2. If scale of input features is different then scale of coefficients\\n   will also be different, making direct comparison between coefficient values\\n   incorrect.\\n3. Depending on regularization, rare features sometimes may have high\\n   coefficients; this doesn't mean they contribute much to the\\n   classification result for most examples.\\n\", error=None, method='linear model', is_regression=False, targets=[TargetExplanation(target=True, feature_weights=FeatureWeights(pos=[FeatureWeight(feature='pos-shape=SYM', weight=1.0294256259777552, std=None, value=None), FeatureWeight(feature='pred-tense=Past', weight=0.514867980774007, std=None, value=None), FeatureWeight(feature='has-date', weight=0.4293019431604141, std=None, value=None), FeatureWeight(feature='pos-shape=ADJ_PRON', weight=0.4083190163073635, std=None, value=None), FeatureWeight(feature='pos-shape=VERB_PROPN_PROPN', weight=0.4080700825968286, std=None, value=None), FeatureWeight(feature='pos-shape=VERB_PRON_PRON', weight=0.40317279849558907, std=None, value=None), FeatureWeight(feature='pos-shape=ADJ_PROPN', weight=0.40269593441299467, std=None, value=None), FeatureWeight(feature='subj-pos=NOUN', weight=0.38301934167158946, std=None, value=None), FeatureWeight(feature='pos-shape=ADJ_NOUN_PRON', weight=0.37603703606637723, std=None, value=None), FeatureWeight(feature='pos-shape=VERB_NUM', weight=0.37310990344767686, std=None, value=None), FeatureWeight(feature='pos-shape=X', weight=0.37301150933716265, std=None, value=None), FeatureWeight(feature='subj-pos=PROPN', weight=0.34970698594350935, std=None, value=None), FeatureWeight(feature='pos-shape=VERB_PROPN_PRON', weight=0.3467732444540139, std=None, value=None), FeatureWeight(feature='pos-shape=ADJ_X', weight=0.33529725958901047, std=None, value=None), FeatureWeight(feature='subj-pos=X', weight=0.3102367644407864, std=None, value=None), FeatureWeight(feature='pos-shape=NOUN_NOUN', weight=0.3091707509723667, std=None, value=None)], neg=[FeatureWeight(feature='pred-tense=Fut', weight=-0.9649140968540975, std=None, value=None), FeatureWeight(feature='pos-shape=ADJ_NOUN_NOUN', weight=-0.6775769486374928, std=None, value=None), FeatureWeight(feature='pos-shape=ADV', weight=-0.6017430747643048, std=None, value=None), FeatureWeight(feature='subj-pos=PRON', weight=-0.5619743875302496, std=None, value=None), FeatureWeight(feature='pos-shape=VERB_NUM_NOUN', weight=-0.5383737525266958, std=None, value=None), FeatureWeight(feature='pos-shape=ADV_NOUN', weight=-0.5026617287829553, std=None, value=None), FeatureWeight(feature='<BIAS>', weight=-0.47757323866327145, std=None, value=None), FeatureWeight(feature='pos-shape=VERB_DET', weight=-0.43686725310954694, std=None, value=None), FeatureWeight(feature='pred-aspect=Imp', weight=-0.42910678809317265, std=None, value=None), FeatureWeight(feature='subj-pos=ADJ', weight=-0.4056136985376675, std=None, value=None), FeatureWeight(feature='pos-shape=ADV_NOUN_PRON', weight=-0.3847865198209768, std=None, value=None), FeatureWeight(feature='pos-shape=VERB', weight=-0.3833248922969526, std=None, value=None), FeatureWeight(feature='pos-shape=ADJ_ADJ', weight=-0.3460912876160607, std=None, value=None), FeatureWeight(feature='pos-shape=VERB_ADJ_NOUN', weight=-0.3218879439594693, std=None, value=None)], pos_remaining=25, neg_remaining=24), proba=None, score=None, weighted_spans=None, heatmap=None)], feature_importances=None, decision_tree=None, highlight_spaces=None, transition_features=None, image=None)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfr = clf.get_params()['lrc']\n",
    "vec = clf.get_params()['dict_vect']\n",
    "feature_names = get_feature_names(clfr, vec)\n",
    "\n",
    "eli5.explain_weights(clfr, top=30, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y = get_data(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                                    random_state=42, shuffle = True, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('dict_vect',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=True)),\n",
       "                ('lrc',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=800,\n",
       "                                    multi_class='multinomial', n_jobs=-1,\n",
       "                                    penalty='l2', random_state=42, solver='sag',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.57      0.66       458\n",
      "        True       0.71      0.87      0.78       564\n",
      "\n",
      "    accuracy                           0.73      1022\n",
      "   macro avg       0.75      0.72      0.72      1022\n",
      "weighted avg       0.74      0.73      0.73      1022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_test, clf.predict(data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./classifier.joblib']"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf, './classifier.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
