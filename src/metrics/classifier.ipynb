{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import stanza\n",
    "from tokenize_uk import tokenize_uk\n",
    "import requests\n",
    "from html.parser import HTMLParser\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/articles/test/zelen.json') as f:\n",
    "    test_data = json.load(f)\n",
    "with open('../../data/articles/train/train_it_2.json') as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('uk', processors='tokenize,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=100, solver='sag', n_jobs=-1))])\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spo_old(search_obj):\n",
    "    raw_text, spo = [(k, search_obj[k]) for k in search_obj][0]\n",
    "    subj, pred, obj = [x.lower() if x else x for x in (\n",
    "        spo if spo else [None, None, None])]\n",
    "    return raw_text, subj, pred, obj\n",
    "\n",
    "\n",
    "def get_subj_pred_obj_from_text(text):\n",
    "    res = []\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sentences:\n",
    "            pred = None\n",
    "            subj = None\n",
    "            obj = None\n",
    "            pred = next(((word.id, word.lemma)\n",
    "                         for word in sent.words if word.deprel == 'root' and word.upos == 'VERB'),\n",
    "                        None)\n",
    "            if pred:\n",
    "                subj = next(((word.id, word.lemma) for word in sent.words if word.deprel ==\n",
    "                             'nsubj' and word.head == int(pred[0])), None)\n",
    "                obj = next(((word.id, word.lemma) for word in sent.words if word.deprel ==\n",
    "                            'obj' and word.head == int(pred[0])), None)\n",
    "            if pred:\n",
    "                res.append((sent.text, (subj[1] if subj else None, pred[1], obj[1] if obj else None)))\n",
    "            else:\n",
    "                res.append((sent.text, None))\n",
    "    except Exception as e:\n",
    "        print(f'Failed to create nlp from text that starts with: {text[:50]}')\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_spo(text):\n",
    "    spos = get_subj_pred_obj_from_text(text)\n",
    "    # TODO: handle multiple sentences; perhaps take the one containing most from SPO\n",
    "    raw_text, spo = spos[0] if spos else (text, None)\n",
    "    subj, pred, obj = [x.lower() if x else x for x in (\n",
    "        spo if spo else [None, None, None])]\n",
    "    return raw_text, subj, pred, obj\n",
    "\n",
    "\n",
    "def search_by_token(token, article):\n",
    "    _, t_subj, _, t_obj = get_spo(article['title'][0])\n",
    "    is_found = get_is_match(token, [t_subj, t_obj])\n",
    "    if not is_found:\n",
    "        for sent in article['content']:\n",
    "            _, s_subj, _, s_obj = get_spo(sent)\n",
    "\n",
    "            is_found = get_is_match(token, [s_subj, s_obj])\n",
    "            if is_found:\n",
    "                break\n",
    "    return is_found\n",
    "\n",
    "\n",
    "def search_relevant_articles(search_term, corpus):\n",
    "    res = []\n",
    "    search_tokens = nlp(search_term).sentences[0].words\n",
    "\n",
    "    for article in corpus:\n",
    "        is_found = None\n",
    "        title_obj = article['title'][0]\n",
    "        title, t_subj, _, t_obj = get_spo(article['title'][0])\n",
    "\n",
    "        if len(search_tokens) == 1:\n",
    "            is_found = search_by_token(search_tokens[0], article)\n",
    "        else:\n",
    "            for token in search_tokens:\n",
    "                is_found = search_by_token(token, article)\n",
    "        if is_found:\n",
    "            res.append(\n",
    "                {'url': article['url'], 'date': article['date'], 'title': title})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data_old(corpus):\n",
    "#     features, labels = [], []\n",
    "    \n",
    "#     for article in corpus:\n",
    "#         feat = {}\n",
    "#         title_obj = article['title'][0]\n",
    "#         title, t_subj, _, t_obj = get_spo(title_obj)\n",
    "#         feat['title'] = title\n",
    "#         # TODO:\n",
    "#         # 1) separate feature extractors\n",
    "#         # 2) check Mariana's suggestions in PR\n",
    "#         # 3) annotate more data\n",
    "#         feat['t_subj'] = t_subj or 'NONE'\n",
    "#         features.append(feat)\n",
    "#         labels.append(article['relevant'])\n",
    "        \n",
    "#     return features, labels\n",
    "\n",
    "\n",
    "def get_data(titles):\n",
    "    features, labels = [], []\n",
    "    \n",
    "    for i, (title, is_event) in enumerate(titles):\n",
    "        feat = {}\n",
    "        # 1) separate feature extractors\n",
    "        # 2) check Mariana's suggestions in PR\n",
    "        title, t_subj, _, t_obj = get_spo(title)\n",
    "        feat['title'] = title\n",
    "        feat['t_subj'] = t_subj or 'NONE'\n",
    "        features.append(feat)\n",
    "        labels.append(is_event)\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print('-->', i)\n",
    "        \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_subj_pred_obj(corpus, res_file):\n",
    "#     current_content = []\n",
    "#     with open(res_file, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(current_content, f)\n",
    "\n",
    "#     for i, art in enumerate(corpus):\n",
    "#         title_with_spo = get_subj_pred_obj_text(art['title'])\n",
    "#         content_with_spo = get_subj_pred_obj_text(art['content'])\n",
    "#         res = {\n",
    "#             'url': art['url'],\n",
    "#             'date': art['date'],\n",
    "#             'title': title_with_spo,\n",
    "#             'content': content_with_spo,\n",
    "#             'relevant': art['relevant']\n",
    "#         }\n",
    "#         with open(res_file, 'r', encoding='utf-8') as f:\n",
    "#             current_content = json.load(f)\n",
    "#         current_content.append(res)\n",
    "\n",
    "#         with open(res_file, 'w', encoding='utf-8') as f:\n",
    "#             json.dump(current_content, f, ensure_ascii=False)\n",
    "#         print('>>>', i)\n",
    "#     return current_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_subj_pred_obj(test_data, '../../data/articles/train/train_it_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = get_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Опублікований фільм Рік президента Зеленського'}, {'title': 'У Зеленського \"не дійшли руки\" до всіх обіцянок'}, {'title': 'Зеленський анонсував скорочення правоохоронних органів'}]\n",
      "[True, False, False]\n"
     ]
    }
   ],
   "source": [
    "# print(X[:3])\n",
    "# print(y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotattion_sources(source_path, target_path):\n",
    "    data_files = os.listdir(source_articles_path)\n",
    "    file_count = 0\n",
    "    for file in data_files:\n",
    "        with open(os.path.join(source_path, file)) as f:\n",
    "            art_count = 0\n",
    "            cont = json.load(f)\n",
    "            for i, article in enumerate(cont):\n",
    "                res = []\n",
    "                res.append(article['url'] + '\\n')\n",
    "                res.append('===\\n')\n",
    "                res.append(f'{article[\"title\"]}\\t|\\tREL_{art_count}\\n')\n",
    "                res.append('===\\n')\n",
    "\n",
    "                sents = tokenize_uk.tokenize_sents(article['content'])\n",
    "                for i, sent in enumerate(sents):\n",
    "                    res.append(f'{sent}\\t|\\tREL_{art_count}_{i}\\n')\n",
    "                res.append(f'\\nREL_{art_count}\\n')\n",
    "                res.append('END\\n\\n')\n",
    "                \n",
    "                with open(os.path.join(target_path, f'to_annotate_{file_count}.txt'), 'a') as f:\n",
    "                    f.writelines(res)\n",
    "                    \n",
    "                    if art_count == 99:\n",
    "                        file_count += 1\n",
    "                        art_count = 0\n",
    "                    else:\n",
    "                        art_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_articles_path = '../../data/articles/source_normalized'\n",
    "annotation_source_path = '../../data/articles/for_annotation'\n",
    "make_annotattion_sources(source_articles_path, annotation_source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1372,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def parse_annotated_articles(path):\n",
    "    files = os.listdir(path)\n",
    "    lines = []\n",
    "    raw_lines = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(annotated_articles_path, file)) as f:\n",
    "            reader = csv.reader(f, dialect='excel-tab')\n",
    "            count = 0\n",
    "            miltiline_events_cache = []\n",
    "            is_all_non_event = None\n",
    "            all_non_events = []\n",
    "            start_mark = None\n",
    "\n",
    "            for i, row in enumerate(reader):\n",
    "                if row:\n",
    "                    first_column = row[0]\n",
    "                    next_column = row[1]\n",
    "                    \n",
    "                    if first_column != '===':\n",
    "                        \n",
    "                        if next_column == 'O':\n",
    "                            fst_col_split = first_column.split(' | ')\n",
    "                            \n",
    "                            # The line requires annotation (is either )\n",
    "                            if len(fst_col_split) > 1:\n",
    "                                # The line was annotated, annotation is set on the next line\n",
    "                                if not fst_col_split[1]:\n",
    "                                    mark_row = reader.__next__()\n",
    "                                    is_event = mark_row[1] == 'IS_EVENT'\n",
    "                                    if not start_mark:\n",
    "                                        start_mark = mark_row[0]\n",
    "                                    if is_all_non_event is not None:\n",
    "                                        is_all_non_event = not is_event\n",
    "                                    line = (fst_col_split[0], is_event)\n",
    "                                    if not len(miltiline_events_cache):\n",
    "                                        lines.append(line)\n",
    "                                        if is_all_non_event:\n",
    "                                            all_non_events.append(line)\n",
    "                                    else:\n",
    "                                        miltiline_events_cache.append(line)\n",
    "                                        is_all_non_event = False\n",
    "                                # Not annotated\n",
    "                                else:\n",
    "#                                     lines.append((fst_col_split[0], None))\n",
    "                                    if is_all_non_event:\n",
    "                                        all_non_events.append((fst_col_split[0], False))\n",
    "                                    mark = re.findall('REL_\\d+', fst_col_split[1])\n",
    "                                    if mark:\n",
    "                                        start_mark = mark[0]\n",
    "                                if len(miltiline_events_cache):\n",
    "                                    # We already have some sentences in the cache\n",
    "                                    # Updating their status\n",
    "                                    ee = [miltiline_events_cache[i:i + 2] for i in range(0, len(miltiline_events_cache), 2)]\n",
    "                                    for e in ee:\n",
    "                                        r = []\n",
    "                                        for x in e:\n",
    "                                            a, b = x\n",
    "                                            if a not in r:\n",
    "                                                r.append(a.strip())\n",
    "                                        t = ' '.join(r)\n",
    "                                        lines.append((t, is_event))\n",
    "                                    miltiline_events_cache = []\n",
    "                            else:\n",
    "                                event_text = fst_col_split[0].strip()\n",
    "                                # Append multi-event text if we already have something in the cache\n",
    "                                if event_text and len(miltiline_events_cache):\n",
    "                                    miltiline_events_cache.append((event_text, next_column))\n",
    "                                    is_all_non_event = False\n",
    "                        elif not (row[0].startswith('REL_') or row == 'END'):\n",
    "                            line = (row[0], next_column)\n",
    "                            miltiline_events_cache.append(line)\n",
    "                            is_all_non_event = False\n",
    "                    # We reached the end\n",
    "                    if first_column == start_mark:\n",
    "                        if is_all_non_event and next_column == 'IS_NOT_EVENT':\n",
    "                            lines += all_non_events\n",
    "                        all_non_events = []\n",
    "                count += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1412\n",
      "587\n",
      "825\n"
     ]
    }
   ],
   "source": [
    "annotated_articles_path = '../../data/articles/annotated'\n",
    "annotated_parsed = parse_annotated_articles(annotated_articles_path)\n",
    "truish = [(x, y) for x, y in annotated_parsed if y]\n",
    "falsish = [(x, y) for x, y in annotated_parsed if not y]\n",
    "print(len(annotated_parsed))\n",
    "print(len(truish))\n",
    "print(len(falsish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> 0\n",
      "Failed to create nlp from text that starts with: Батькам нікуди дітей дівати. Кернес не ввів карант\n",
      "--> 200\n",
      "--> 400\n",
      "--> 600\n",
      "--> 800\n",
      "--> 1000\n",
      "--> 1200\n",
      "--> 1400\n"
     ]
    }
   ],
   "source": [
    "X, y = get_data(annotated_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424\n"
     ]
    }
   ],
   "source": [
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1367,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                                    random_state=42, shuffle = True, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1338,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('dict_vect',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=True)),\n",
       "                ('lrc',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='multinomial', n_jobs=-1,\n",
       "                                    penalty='l2', random_state=42, solver='sag',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 1368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.68      0.95      0.79       248\n",
      "        True       0.84      0.36      0.51       176\n",
      "\n",
      "    accuracy                           0.71       424\n",
      "   macro avg       0.76      0.66      0.65       424\n",
      "weighted avg       0.75      0.71      0.67       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_test, clf.predict(data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix glued sentences:\n",
    "# Григорій СуркісСуд зобов'язав ПриватБанк виплатити офшорним компаніям Суркісів депозити\n",
    "# на $250 млн Апеляційний суд Києва відхилив скаргу юристів державного ПриватБанку на рішення\n",
    "# Печерського райсуду про фактичне стягнення з банку $250 млн, які зберігалися на депозитах шести компаній,\n",
    "#пов'язаних із бізнесменами Григорієм та Ігорем Суркісами в кіпрській філії ПриватБанку до націоналізації. | REL_87_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorythm of parsing annotated data\n",
    "# 1) if all the article marked as IS_NO_EVENT and there are no other marks inside - mark every sentense as NO_EVENT\n",
    "# 2) if a block contains more than one sentence - split into multiple sentences by annotated split object (DATE annotation at the sentence beginning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../data/articles/annotated/parsed.json', 'w') as f:\n",
    "#     json.dump(annotated_parsed, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Зеленський просить Мінфін почати переговори з МВФ про допомогу через коронавірус ('Зеленський', 'просити', 'мінфін')\n"
     ]
    }
   ],
   "source": [
    "a = [('Зеленський просить Мінфін почати переговори з МВФ про допомогу через коронавірус', ('Зеленський', 'просити', 'мінфін'))]\n",
    "b, c = a[0] if a else ('fdfdf', None)\n",
    "print(b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
