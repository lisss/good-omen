{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import stanza\n",
    "from tokenize_uk import tokenize_uk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/articles/test/zelen.json') as f:\n",
    "    test_data = json.load(f)\n",
    "with open('../../data/articles/train/train_it_2.json') as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('uk', processors='tokenize,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=100, solver='sag', n_jobs=-1))])\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotattion_sources(source_path, target_path):\n",
    "    data_files = os.listdir(source_articles_path)\n",
    "    file_count = 0\n",
    "    for file in data_files:\n",
    "        with open(os.path.join(source_path, file)) as f:\n",
    "            art_count = 0\n",
    "            cont = json.load(f)\n",
    "            for i, article in enumerate(cont):\n",
    "                res = []\n",
    "                res.append(article['url'] + '\\n')\n",
    "                res.append('===\\n')\n",
    "                res.append(f'{article[\"title\"]}\\t|\\tREL_{art_count}\\n')\n",
    "                res.append('===\\n')\n",
    "\n",
    "                sents = tokenize_uk.tokenize_sents(article['content'])\n",
    "                for i, sent in enumerate(sents):\n",
    "                    res.append(f'{sent}\\t|\\tREL_{art_count}_{i}\\n')\n",
    "                res.append(f'\\nREL_{art_count}\\n')\n",
    "                res.append('END\\n\\n')\n",
    "                \n",
    "                with open(os.path.join(target_path, f'to_annotate_{file_count}.txt'), 'a') as f:\n",
    "                    f.writelines(res)\n",
    "                    \n",
    "                    if art_count == 99:\n",
    "                        file_count += 1\n",
    "                        art_count = 0\n",
    "                    else:\n",
    "                        art_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1611,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "''' Parse annotated data\n",
    "\n",
    "algorythm of parsing annotated data\n",
    "1) if all the article marked as IS_NO_EVENT and there are no other marks inside -\n",
    "mark every sentense as NO_EVENT\n",
    "2) if a block contains more than one sentence - split into multiple sentences\n",
    "by annotated split object (DATE annotation at the sentence beginning)\n",
    "'''\n",
    "\n",
    "\n",
    "def parse_annotated_articles(annotated_articles_path):\n",
    "    files = os.listdir(annotated_articles_path)\n",
    "    lines = []\n",
    "    raw_lines = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(annotated_articles_path, file)) as f:\n",
    "            reader = csv.reader(f, dialect='excel-tab')\n",
    "            count = 0\n",
    "            miltiline_events_cache = []\n",
    "            is_all_non_event = None\n",
    "            all_non_events = []\n",
    "            start_mark = None\n",
    "\n",
    "            for i, row in enumerate(reader):\n",
    "                if row:\n",
    "                    first_column = row[0]\n",
    "                    next_column = row[1]\n",
    "\n",
    "                    if first_column != '===':\n",
    "\n",
    "                        if next_column == 'O':\n",
    "                            fst_col_split = first_column.split(' | ')\n",
    "\n",
    "                            # The line was required to be annotated annotation\n",
    "                            if len(fst_col_split) > 1:\n",
    "                                # The line was annotated, annotation is set on the next line\n",
    "                                if not fst_col_split[1]:\n",
    "                                    mark_row = reader.__next__()\n",
    "                                    is_event = mark_row[1] == 'IS_EVENT'\n",
    "                                    if not start_mark:\n",
    "                                        start_mark = mark_row[0]\n",
    "                                    if is_all_non_event is not None:\n",
    "                                        is_all_non_event = not is_event\n",
    "                                    line = (fst_col_split[0], is_event)\n",
    "                                    if not len(miltiline_events_cache):\n",
    "                                        lines.append(line)\n",
    "                                        if is_all_non_event:\n",
    "                                            all_non_events.append(line)\n",
    "                                    else:\n",
    "                                        miltiline_events_cache.append(line)\n",
    "                                        is_all_non_event = False\n",
    "                                # Not annotated\n",
    "                                else:\n",
    "                                    if is_all_non_event:\n",
    "                                        all_non_events.append(\n",
    "                                            (fst_col_split[0], False))\n",
    "                                    mark = re.findall(\n",
    "                                        'REL_\\\\d+', fst_col_split[1])\n",
    "                                    if mark:\n",
    "                                        start_mark = mark[0]\n",
    "                                if len(miltiline_events_cache):\n",
    "                                    # We already have some sentences in the cache\n",
    "                                    # Updating their status\n",
    "                                    ee = [miltiline_events_cache[i:i + 2]\n",
    "                                          for i in range(0, len(miltiline_events_cache), 2)]\n",
    "                                    for e in ee:\n",
    "                                        r = []\n",
    "                                        for x in e:\n",
    "                                            a, b = x\n",
    "                                            if a not in r:\n",
    "                                                r.append(a.strip())\n",
    "                                        t = ' '.join(r)\n",
    "                                        lines.append((t, is_event))\n",
    "                                    miltiline_events_cache = []\n",
    "                            else:\n",
    "                                event_text = fst_col_split[0].strip()\n",
    "                                # Append multi-event text if we already have something in the cache\n",
    "                                if event_text and len(miltiline_events_cache):\n",
    "                                    miltiline_events_cache.append(\n",
    "                                        (event_text, next_column))\n",
    "                                    is_all_non_event = False\n",
    "                        elif not (row[0].startswith('REL_') or row == 'END'):\n",
    "                            line = (row[0], next_column)\n",
    "                            miltiline_events_cache.append(line)\n",
    "                            is_all_non_event = False\n",
    "                    # We reached the end\n",
    "                    if first_column == start_mark:\n",
    "                        if is_all_non_event and next_column == 'IS_NOT_EVENT':\n",
    "                            lines += all_non_events\n",
    "                        all_non_events = []\n",
    "                count += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1612,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_features(feats_string):\n",
    "    res = {}\n",
    "    feats = feats_string.split('|')\n",
    "    for feat in feats:\n",
    "        k, v = feat.split('=')\n",
    "        res[k] = v\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_subj_pred_obj_from_text(text):\n",
    "    res = []\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sentences:\n",
    "            spo = {}\n",
    "            pred = None\n",
    "            subj = None\n",
    "            obj = None\n",
    "            root = next((word\n",
    "                         for word in sent.words if word.deprel == 'root' and word.upos == 'VERB'),\n",
    "                        None)\n",
    "            if root:\n",
    "                root_conj = next((word for word in sent.words if word.deprel ==\n",
    "                                  'conj' and word.head == int(root.id)), None)\n",
    "\n",
    "                subj = next((word for word in sent.words if word.deprel ==\n",
    "                             'nsubj' and word.head == int(root.id)), None)\n",
    "                obj = next(((word.id, word.lemma) for word in sent.words if word.deprel ==\n",
    "                            'obj' and word.head == int(root_conj.id if root_conj else root.id)),\n",
    "                           None)\n",
    "\n",
    "                spo['subj'] = subj.lemma if subj else None\n",
    "                spo['root'] = root\n",
    "                spo['root-conj'] = root_conj\n",
    "                spo['obj'] = obj[1] if obj else None\n",
    "                if subj:\n",
    "                    conj = next((word.lemma for word in sent.words if word.deprel ==\n",
    "                                 'conj' and word.head == int(subj.id)), None)\n",
    "                    spo['subj-conj'] = conj\n",
    "\n",
    "            res.append((sent.text, spo))\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f'Failed to create nlp from text that starts with: {text[:50]} {e}')\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_spo(text):\n",
    "    spos = get_subj_pred_obj_from_text(text)\n",
    "    # TODO: handle multiple sentences; perhaps take the one containing most from SPO\n",
    "    raw_text, spo = spos[0] if spos else (text, {})\n",
    "    return raw_text, spo\n",
    "\n",
    "\n",
    "def get_data(titles):\n",
    "    features, labels = [], []\n",
    "\n",
    "    for i, (title, is_event) in enumerate(titles):\n",
    "        feat = {}\n",
    "        title, spo = get_spo(title)\n",
    "        if spo:\n",
    "            root = spo['root']\n",
    "            root_conj = spo.get('root-conj')\n",
    "\n",
    "            pred_features = parse_features(root.feats)\n",
    "\n",
    "            subj_conj = spo.get('subj-conj')\n",
    "\n",
    "            if subj_conj:\n",
    "                feat['subj'] = f'{spo[\"subj\"]}_{subj_conj}'\n",
    "            else:\n",
    "                feat['subj'] = spo.get('subj') or 'NONE'\n",
    "#             feat['subj'] = spo.get('subj') or 'NONE'\n",
    "\n",
    "            if root_conj:\n",
    "                feat['pred'] = f'{root.lemma}_{root_conj.lemma}'\n",
    "                pred_conj_features = parse_features(root_conj.feats)\n",
    "                feat['pred-conj-tense'] = pred_conj_features.get(\n",
    "                    'Tense') or 'NONE'\n",
    "            else:\n",
    "                feat['pred'] = root.lemma\n",
    "#             feat['pred'] = root.lemma\n",
    "            feat['obj'] = spo.get('obj') or 'NONE'\n",
    "            feat['pred-tense'] = pred_features.get('Tense') or 'NONE'\n",
    "        features.append(feat)\n",
    "        labels.append(is_event)\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print('-->', i)\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_articles_path = '../../data/articles/source_normalized'\n",
    "annotation_source_path = '../../data/articles/for_annotation'\n",
    "make_annotattion_sources(source_articles_path, annotation_source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: 2801\n",
      "Is event: 1330\n",
      "Is not event: 1471\n"
     ]
    }
   ],
   "source": [
    "annotated_articles_path = '../../data/articles/annotated'\n",
    "annotated_parsed = parse_annotated_articles(annotated_articles_path)\n",
    "annotated_parsed = list(set(annotated_parsed))\n",
    "truish = [(x, y) for x, y in annotated_parsed if y]\n",
    "falsish = [(x, y) for x, y in annotated_parsed if not y]\n",
    "print('All:', len(annotated_parsed))\n",
    "print('Is event:', len(truish))\n",
    "print('Is not event:', len(falsish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> 0\n",
      "--> 500\n",
      "--> 1000\n",
      "Failed to create nlp from text that starts with: Батькам нікуди дітей дівати. Кернес не ввів карант \n",
      "Failed to create nlp from text that starts with: Там за півдоби зареєстрували понад 80 нових випадк \n",
      "--> 1500\n",
      "--> 2000\n",
      "--> 2500\n"
     ]
    }
   ],
   "source": [
    "X, y = get_data(annotated_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1616,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                                    random_state=42, shuffle = True, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1338,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('dict_vect',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=True)),\n",
       "                ('lrc',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='multinomial', n_jobs=-1,\n",
       "                                    penalty='l2', random_state=42, solver='sag',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 1617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      0.77      0.77       442\n",
      "        True       0.74      0.74      0.74       399\n",
      "\n",
      "    accuracy                           0.75       841\n",
      "   macro avg       0.75      0.75      0.75       841\n",
      "weighted avg       0.75      0.75      0.75       841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_test, clf.predict(data_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
