{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import dateparser\n",
    "import datetime\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "import stanza\n",
    "from tokenize_uk import tokenize_uk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/articles/test/zelen.json') as f:\n",
    "    test_data = json.load(f)\n",
    "with open('../../data/articles/train/train_it_2.json') as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 18:37:33 INFO: Loading these models for language: uk (Ukrainian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | iu      |\n",
      "| pos       | iu      |\n",
      "| lemma     | iu      |\n",
      "| depparse  | iu      |\n",
      "=======================\n",
      "\n",
      "2020-06-09 18:37:33 INFO: Use device: cpu\n",
      "2020-06-09 18:37:33 INFO: Loading: tokenize\n",
      "2020-06-09 18:37:33 INFO: Loading: pos\n",
      "2020-06-09 18:37:34 INFO: Loading: lemma\n",
      "2020-06-09 18:37:34 INFO: Loading: depparse\n",
      "2020-06-09 18:37:35 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('uk', processors='tokenize,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=800, solver='sag', n_jobs=-1))])\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotattion_sources(source_path, target_path):\n",
    "    data_files = os.listdir(source_articles_path)\n",
    "    file_count = 0\n",
    "    for file in data_files:\n",
    "        with open(os.path.join(source_path, file)) as f:\n",
    "            art_count = 0\n",
    "            cont = json.load(f)\n",
    "            for i, article in enumerate(cont):\n",
    "                res = []\n",
    "                res.append(article['url'] + '\\n')\n",
    "                res.append('===\\n')\n",
    "                res.append(f'{article[\"title\"]}\\t|\\tREL_{art_count}\\n')\n",
    "                res.append('===\\n')\n",
    "\n",
    "                sents = tokenize_uk.tokenize_sents(article['content'])\n",
    "                for i, sent in enumerate(sents):\n",
    "                    res.append(f'{sent}\\t|\\tREL_{art_count}_{i}\\n')\n",
    "                res.append(f'\\nREL_{art_count}\\n')\n",
    "                res.append('END\\n\\n')\n",
    "                \n",
    "                with open(os.path.join(target_path, f'to_annotate_{file_count}.txt'), 'a') as f:\n",
    "                    f.writelines(res)\n",
    "                    \n",
    "                    if art_count == 99:\n",
    "                        file_count += 1\n",
    "                        art_count = 0\n",
    "                    else:\n",
    "                        art_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "''' Parse annotated data\n",
    "\n",
    "algorythm of parsing annotated data\n",
    "1) if all the article marked as IS_NO_EVENT and there are no other marks inside -\n",
    "mark every sentense as NO_EVENT\n",
    "2) if a block contains more than one sentence - split into multiple sentences\n",
    "by annotated split object (DATE annotation at the sentence beginning)\n",
    "'''\n",
    "\n",
    "\n",
    "def parse_annotated_articles(annotated_articles_path):\n",
    "    files = os.listdir(annotated_articles_path)\n",
    "    lines = []\n",
    "    raw_lines = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(annotated_articles_path, file)) as f:\n",
    "            reader = csv.reader(f, dialect='excel-tab')\n",
    "            count = 0\n",
    "            miltiline_events_cache = []\n",
    "            is_all_non_event = None\n",
    "            all_non_events = []\n",
    "            start_mark = None\n",
    "\n",
    "            for i, row in enumerate(reader):\n",
    "                if row:\n",
    "                    first_column = row[0]\n",
    "                    next_column = row[1]\n",
    "\n",
    "                    if first_column != '===':\n",
    "\n",
    "                        if next_column == 'O':\n",
    "                            fst_col_split = first_column.split(' | ')\n",
    "\n",
    "                            # The line was required to be annotated annotation\n",
    "                            if len(fst_col_split) > 1:\n",
    "                                # The line was annotated, annotation is set on the next line\n",
    "                                if not fst_col_split[1]:\n",
    "                                    mark_row = reader.__next__()\n",
    "                                    is_event = mark_row[1] == 'IS_EVENT'\n",
    "                                    if not start_mark:\n",
    "                                        start_mark = mark_row[0]\n",
    "                                    if is_all_non_event is not None:\n",
    "                                        is_all_non_event = not is_event\n",
    "                                    line = (fst_col_split[0], is_event)\n",
    "                                    if not len(miltiline_events_cache):\n",
    "                                        lines.append(line)\n",
    "                                        if is_all_non_event:\n",
    "                                            all_non_events.append(line)\n",
    "                                    else:\n",
    "                                        miltiline_events_cache.append(line)\n",
    "                                        is_all_non_event = False\n",
    "                                # Not annotated\n",
    "                                else:\n",
    "                                    if is_all_non_event:\n",
    "                                        all_non_events.append(\n",
    "                                            (fst_col_split[0], False))\n",
    "                                    mark = re.findall(\n",
    "                                        'REL_\\\\d+', fst_col_split[1])\n",
    "                                    if mark:\n",
    "                                        start_mark = mark[0]\n",
    "                                if len(miltiline_events_cache):\n",
    "                                    # We already have some sentences in the cache\n",
    "                                    # Updating their status\n",
    "                                    ee = [miltiline_events_cache[i:i + 2]\n",
    "                                          for i in range(0, len(miltiline_events_cache), 2)]\n",
    "                                    for e in ee:\n",
    "                                        r = []\n",
    "                                        for x in e:\n",
    "                                            a, b = x\n",
    "                                            if a not in r:\n",
    "                                                r.append(a.strip())\n",
    "                                        t = ' '.join(r)\n",
    "                                        lines.append((t, is_event))\n",
    "                                    miltiline_events_cache = []\n",
    "                            else:\n",
    "                                event_text = fst_col_split[0].strip()\n",
    "                                # Append multi-event text if we already have something in the cache\n",
    "                                if event_text and len(miltiline_events_cache):\n",
    "                                    miltiline_events_cache.append(\n",
    "                                        (event_text, next_column))\n",
    "                                    is_all_non_event = False\n",
    "                        elif not (row[0].startswith('REL_') or row == 'END'):\n",
    "                            line = (row[0], next_column)\n",
    "                            miltiline_events_cache.append(line)\n",
    "                            is_all_non_event = False\n",
    "                    # We reached the end\n",
    "                    if first_column == start_mark:\n",
    "                        if is_all_non_event and next_column == 'IS_NOT_EVENT':\n",
    "                            lines += all_non_events\n",
    "                        all_non_events = []\n",
    "                count += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(doc):\n",
    "    print(*[f'id: {word.id}\\tword: {word.text}\\tPOS:{word.upos}\\t\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "\n",
    "\n",
    "def get_all_docs(annotated_texts):\n",
    "    docs = []\n",
    "    for i, (text, is_event) in enumerate(annotated_texts):\n",
    "        try:\n",
    "            doc = nlp(text)\n",
    "            docs.append((doc, is_event))\n",
    "        except Exception as e:\n",
    "            print(f'Failed to create nlp from text that starts with: {text[:50]} {e}')\n",
    "        if i % 500 == 0:\n",
    "            print('-->', i)\n",
    "    return docs\n",
    "        \n",
    "\n",
    "def parse_features(feats_string):\n",
    "    res = {}\n",
    "    feats = feats_string.split('|')\n",
    "    for feat in feats:\n",
    "        k, v = feat.split('=')\n",
    "        res[k] = v\n",
    "    return res\n",
    "\n",
    "\n",
    "def find_dates(string, is_future=False):\n",
    "    valid_months = ['січня', 'січ.','лютого','лют.','березня','берез.','квітня','квіт.',\n",
    "                'травня','трав.','червня','черв.','липня','лип.','серпня','серп.',\n",
    "               'вересня','верес.','жовтня','жовт.','листопада','листоп.','грудня','груд.']\n",
    "    mon_regex_str = '|'.join(valid_months).replace('.', '\\.')\n",
    "    regex = '(\\s\\d{4}\\s|(\\d+ (' + mon_regex_str + ')(\\s\\d{4})?)|\\d{2,4}-\\d{2}-\\d{2,4}|\\d{2}.\\d{2}.\\d{2,4}|\\d{2}\\/\\d{2}\\/\\d{2,4})'\n",
    "    matches = re.findall(regex, string, re.IGNORECASE)\n",
    "    dates = []\n",
    "    for match in matches:\n",
    "        date = match[0].strip()\n",
    "        if len(date) == 4:\n",
    "            curr_year = datetime.datetime.now().year\n",
    "            # fragile thing, it may predict date if it's actually some other 4-digit stuff\n",
    "            if int(date) <= curr_year or is_future:\n",
    "                dates.append(date)\n",
    "        else:\n",
    "            dates.append(date)\n",
    "    return dates\n",
    "\n",
    "\n",
    "def get_doc_core_members(doc):\n",
    "    res = []\n",
    "    \n",
    "    adv_final = ['вперше', 'нарешті', 'врешті', 'вчора', 'сьогодні', 'позавчора']\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        spo = {}\n",
    "        pred = None\n",
    "        subj = None\n",
    "        obj = None\n",
    "        \n",
    "        num_words = len(sent.words)\n",
    "        \n",
    "        root = next((word\n",
    "                     for word in sent.words if word.deprel == 'root'),\n",
    "                    None)\n",
    "        if not doc.text.strip() or num_words == 2 and sent.words[num_words - 1].upos == 'PUNCT':\n",
    "            continue\n",
    "        # FIXME: iterate only once\n",
    "        if root:\n",
    "            root_conj = next((word for word in sent.words if word.deprel ==\n",
    "                              'conj' and word.head == int(root.id)), None)\n",
    "            root_mod = next((word for word in sent.words if word.deprel ==\n",
    "                              'advmod' and word.upos == 'PART' and word.head == int(root.id)), None)\n",
    "\n",
    "            subj = next((word for word in sent.words if word.deprel ==\n",
    "                         'nsubj' and word.head == int(root.id)), None)\n",
    "            obj = next((word for word in sent.words if word.deprel ==\n",
    "                        'obj' and word.head == int(root_conj.id if root_conj else root.id)),\n",
    "                       None)\n",
    "            c_conj = next((word\n",
    "                     for word in sent.words if word.upos == 'CCONJ' and sent.words[int(word.id) - 2].upos == 'PUNCT'),\n",
    "                    None)\n",
    "            root_adv_final = next((word for word in sent.words if word.deprel ==\n",
    "                        'advmod' and word.upos == 'ADV' and word.head == int(root.id) \\\n",
    "                and word.lemma in adv_final),\n",
    "                       None)\n",
    "            root_xcomp = next((word for word in sent.words if word.deprel ==\n",
    "                              'xcomp' and word.head == int(root.id)), None)\n",
    "            root_xcomp_noun = next((word for word in sent.words if word.deprel == 'xcomp:sp' \\\n",
    "                               and word.upos == 'NOUN' \\\n",
    "                              and word.head == int(root.id)),\n",
    "                                    None)\n",
    "\n",
    "            spo['subj'] = subj\n",
    "            spo['root'] = root\n",
    "            spo['root-conj'] = root_conj\n",
    "            spo['obj'] = obj\n",
    "            spo['root_mod'] = root_mod\n",
    "            spo['c_conj'] = c_conj\n",
    "            spo['root_adv_final'] = root_adv_final\n",
    "            spo['root_xcomp'] = root_xcomp\n",
    "            spo['root_xcomp_noun'] = root_xcomp_noun\n",
    "            if subj:\n",
    "                subj_conj = next((word for word in sent.words if word.deprel ==\n",
    "                             'conj' and (word.upos == 'NOUN' or word.upos == 'PRON') \\\n",
    "                                  and word.head == int(subj.id)), None)\n",
    "                spo['subj-conj'] = subj_conj\n",
    "                if subj_conj:\n",
    "                    subj_conj_verb = next((word for word in sent.words if word.upos ==\n",
    "                        'VERB' and word.head == int(subj_conj.id)),\n",
    "                       None)\n",
    "                    spo['subj-conj-verb'] = subj_conj_verb\n",
    "\n",
    "        res.append((sent.text, spo))\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def get_features(doc):\n",
    "    features = []\n",
    "    \n",
    "    predicate_special = ['допустити', 'думати', 'припустити', 'відреагувати', 'пояснити',\n",
    "                     'сказати', 'заявити', 'повідомити', 'повідомляти', 'розповісти',\n",
    "                      'розповідати', 'рекомендувати', 'порекомендувати', 'мати', 'стати', 'почати']\n",
    "\n",
    "    spos = get_doc_core_members(doc)\n",
    "#     print(spos)\n",
    "    for sent_text, spo in spos:\n",
    "        feat = {}\n",
    "        if spo:\n",
    "            root = spo['root']\n",
    "            root_conj = spo.get('root-conj')\n",
    "            root_adv_final = spo.get('root_adv_final')\n",
    "            root_xcomp = spo.get('root_xcomp')\n",
    "            root_xcomp_noun = spo.get('root_xcomp_noun')\n",
    "            subj = spo.get('subj')\n",
    "            subj_conj = spo.get('subj-conj')\n",
    "            obj = spo.get('obj')\n",
    "\n",
    "            dates = find_dates(doc.text, True)\n",
    "\n",
    "            if root.feats:\n",
    "                pred_features = parse_features(root.feats)\n",
    "            else:\n",
    "                pred_features = {}\n",
    "\n",
    "            feat['subj'] = 'SUBJ' if subj else 'NONE'\n",
    "            feat['has-date'] = len(dates) > 0\n",
    "#             feat['c_conj'] = spo.get('c_conj') is not None\n",
    "#             # TODO: mb remove these\n",
    "#             feat['root_adv_final'] = root_adv_final is not None\n",
    "#             feat['pred-special'] = root.lemma in predicate_special\n",
    "            if pred_features.get('Tense') == 'Past':\n",
    "                feat['root_xcomp'] = root_xcomp is not None\n",
    "                if root_xcomp:\n",
    "                    feat['root_xcomp_pos'] = root_xcomp.upos\n",
    "\n",
    "            if subj:\n",
    "#                 subj_features = parse_features(subj.feats)\n",
    "#                 feat['subj-animacy'] = subj_features.get('Animacy') or 'NONE'\n",
    "                feat['subj-pos'] = subj.upos\n",
    "#             if subj_conj:\n",
    "#                 feat['subj-conj'] = 'SUBJ_CONJ'\n",
    "#                 subj_conj_features = parse_features(subj_conj.feats)\n",
    "#                 feat['subj-conj-animacy'] = subj_conj_features.get('Animacy') or 'NONE'\n",
    "\n",
    "#             if root_conj and root_conj.upos == 'VERB':\n",
    "#                 pred_conj_features = parse_features(root_conj.feats)\n",
    "#                 feat['pred-conj-tense'] = pred_conj_features.get(\n",
    "#                     'Tense') or 'NONE'\n",
    "\n",
    "            feat['pred'] = root.lemma\n",
    "            feat['pred-pos'] = root.upos\n",
    "            feat['obj'] = 'OBJ' if obj else 'NONE'\n",
    "            if obj:\n",
    "                feat['obj-pos'] = obj.upos\n",
    "#             if root.upos != 'VERB':\n",
    "#                 print(root.text, root.upos, root.feats, doc.text)\n",
    "            if root.upos == 'VERB':\n",
    "                feat['pred-tense'] = pred_features.get('Tense') or 'NONE'\n",
    "                feat['pred-aspect'] = pred_features.get('Aspect') or 'NONE'\n",
    "            if root.upos == 'NOUN' or root.upos == 'PROPN':\n",
    "                feat['pred-anim'] = pred_features.get('Animacy') or 'NONE'\n",
    "                feat['pred-abbr'] = pred_features.get('Abbr') or 'NONE'\n",
    "            \n",
    "            features.append(feat)\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def get_data(docs):\n",
    "    def _get_spo_shape(s, p, o):\n",
    "        ids = [p.id]\n",
    "        if s:\n",
    "            ids.append(s.id)\n",
    "        if o:\n",
    "            ids.append(o.id)\n",
    "        indexes = [str(y) for x, y in sorted([(x, i) for i, x in enumerate(ids)])]\n",
    "        \n",
    "        return '_'.join(indexes)\n",
    "\n",
    "    features, labels = [], []\n",
    "\n",
    "    for doc, is_event in docs:\n",
    "        feats = get_features(doc)\n",
    "        for feat in feats:\n",
    "            features.append(feat)\n",
    "            labels.append(is_event if feat else False)\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_articles_path = '../../data/articles/source_normalized'\n",
    "# annotation_source_path = '../../data/articles/for_annotation'\n",
    "# make_annotattion_sources(source_articles_path, annotation_source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: 2720\n",
      "Is event: 1502\n",
      "Is not event: 1218\n"
     ]
    }
   ],
   "source": [
    "annotated_articles_path = '../../data/articles/annotated'\n",
    "annotated_parsed = parse_annotated_articles(annotated_articles_path)\n",
    "annotated_parsed = list(set(annotated_parsed))\n",
    "truish = [(x, y) for x, y in annotated_parsed if y]\n",
    "falsish = [(x, y) for x, y in annotated_parsed if not y]\n",
    "print('All:', len(annotated_parsed))\n",
    "print('Is event:', len(truish))\n",
    "print('Is not event:', len(falsish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2497,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../data/articles/annotated_parsed.json', 'w') as f:\n",
    "#     json.dump(annotated_parsed, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> 0\n",
      "--> 500\n",
      "--> 1000\n",
      "--> 1500\n",
      "Failed to create nlp from text that starts with: Батькам нікуди дітей дівати. Кернес не ввів карант \n",
      "--> 2000\n",
      "--> 2500\n"
     ]
    }
   ],
   "source": [
    "all_docs = get_all_docs(annotated_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = get_data(all_docs[300:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y = get_data(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                                    random_state=42, shuffle = True, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('dict_vect',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=True)),\n",
       "                ('lrc',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=800,\n",
       "                                    multi_class='multinomial', n_jobs=-1,\n",
       "                                    penalty='l2', random_state=42, solver='sag',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      0.70      0.73       395\n",
      "        True       0.77      0.82      0.80       483\n",
      "\n",
      "    accuracy                           0.77       878\n",
      "   macro avg       0.77      0.76      0.76       878\n",
      "weighted avg       0.77      0.77      0.77       878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_test, clf.predict(data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subj': 'NONE', 'has-date': False, 'pred': 'ЗМІ', 'pred-pos': 'NOUN', 'obj': 'NONE', 'pred-anim': 'Inan', 'pred-abbr': 'Yes'}]\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./classifier.joblib']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_is_event(sent, clf):\n",
    "    doc = nlp(sent)\n",
    "    features = get_features(doc)\n",
    "    print(features)\n",
    "    return clf.predict(features)[0]\n",
    "    \n",
    "\n",
    "is_ev = predict_is_event('ЗМІ: ДБР дозволили примусовий привід Порошенка', clf)\n",
    "print(is_ev)\n",
    "\n",
    "dump(clf, './classifier.joblib')\n",
    "# clf2 = load('./classifier.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'ЗМІ: ДБР дозволили примусовий привід Порошенка'\n",
    "# find path to 'буде' and check it's tense\n",
    "s = 'Політична партія Рух нових сил Михайла Саакашвілі буде під номером 22 у виборчому бюлетені на позачергових парламентських виборах.'\n",
    "s = 'В Україні буде сухо і тепло, але з \"ложечкою дьогтю\"'\n",
    "s = 'З 7 квітня державний кордон України можна буде перетнути лише автотранспортом і лише у 19 пунктах пропуску.'\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_head(tokens, word, root, deprel):\n",
    "    head = int(word.head)\n",
    "    is_found = False\n",
    "    while not is_found or head != 0:\n",
    "        if tokens[head - 2].deprel == deprel and int(tokens[head - 2].head) == root:\n",
    "            print('&&&', tokens[head - 2], word)\n",
    "            is_found = True\n",
    "            return tokens[head - 2], word\n",
    "\n",
    "        head = int(tokens[head - 2].id)\n",
    "    return is_found\n",
    "\n",
    "for sent in doc.sentences:\n",
    "    root = next((word\n",
    "                     for word in sent.words if word.deprel == 'root' and word.upos == 'VERB'),\n",
    "                    None)\n",
    "#     xc = [word for word in sent.words if get_head(sent.words, word, int(root.id), 'xcomp:sp') \\\n",
    "#                                and word.upos == 'NOUN']\n",
    "\n",
    "#     print('>>', xc)\n",
    "\n",
    "#     for word in sent.words:\n",
    "#         print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
