{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import dateparser\n",
    "import datetime\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import eli5\n",
    "from eli5.sklearn.utils import get_feature_names\n",
    "import pickle\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import stanza\n",
    "from tokenize_uk import tokenize_uk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-12 13:03:19 INFO: Loading these models for language: uk (Ukrainian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | iu      |\n",
      "| pos       | iu      |\n",
      "| lemma     | iu      |\n",
      "| depparse  | iu      |\n",
      "=======================\n",
      "\n",
      "2020-06-12 13:03:19 INFO: Use device: cpu\n",
      "2020-06-12 13:03:19 INFO: Loading: tokenize\n",
      "2020-06-12 13:03:19 INFO: Loading: pos\n",
      "2020-06-12 13:03:20 INFO: Loading: lemma\n",
      "2020-06-12 13:03:20 INFO: Loading: depparse\n",
      "2020-06-12 13:03:21 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('uk', processors='tokenize,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=800, solver='sag', n_jobs=-1))])\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotattion_sources(source_path, target_path):\n",
    "    data_files = os.listdir(source_articles_path)\n",
    "    file_count = 0\n",
    "    for file in data_files:\n",
    "        with open(os.path.join(source_path, file)) as f:\n",
    "            art_count = 0\n",
    "            cont = json.load(f)\n",
    "            for i, article in enumerate(cont):\n",
    "                res = []\n",
    "                res.append(article['url'] + '\\n')\n",
    "                res.append('===\\n')\n",
    "                res.append(f'{article[\"title\"]}\\t|\\tREL_{art_count}\\n')\n",
    "                res.append('===\\n')\n",
    "\n",
    "                sents = tokenize_uk.tokenize_sents(article['content'])\n",
    "                for i, sent in enumerate(sents):\n",
    "                    res.append(f'{sent}\\t|\\tREL_{art_count}_{i}\\n')\n",
    "                res.append(f'\\nREL_{art_count}\\n')\n",
    "                res.append('END\\n\\n')\n",
    "                \n",
    "                with open(os.path.join(target_path, f'to_annotate_{file_count}.txt'), 'a') as f:\n",
    "                    f.writelines(res)\n",
    "                    \n",
    "                    if art_count == 99:\n",
    "                        file_count += 1\n",
    "                        art_count = 0\n",
    "                    else:\n",
    "                        art_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "''' Parse annotated data\n",
    "\n",
    "algorythm of parsing annotated data\n",
    "1) if all the article marked as IS_NO_EVENT and there are no other marks inside -\n",
    "mark every sentense as NO_EVENT\n",
    "2) if a block contains more than one sentence - split into multiple sentences\n",
    "by annotated split object (DATE annotation at the sentence beginning)\n",
    "'''\n",
    "\n",
    "\n",
    "def trim_indirect_speech(text):\n",
    "    stops = ['Курс НБУ: ']\n",
    "    \n",
    "    text_trimmed = text\n",
    "\n",
    "    if ', -' in text_trimmed:\n",
    "        text_trimmed = re.sub('(\\\"|»)?\\,\\s-\\s.*', '', text_trimmed)\n",
    "    text_trimmed = re.sub('(^\\w+(-\\w+)?((\\s+\\w+(-\\w+)?){1})?):\\s', '', text_trimmed)\n",
    "    for stop in stops:\n",
    "        text_trimmed = text_trimmed.replace(stop, '')\n",
    "    return text_trimmed\n",
    "\n",
    "\n",
    "def parse_annotated_articles(annotated_articles_path):\n",
    "    files = os.listdir(annotated_articles_path)\n",
    "    lines = []\n",
    "    raw_lines = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(annotated_articles_path, file)) as f:\n",
    "            reader = csv.reader(f, dialect='excel-tab')\n",
    "            count = 0\n",
    "            miltiline_events_cache = []\n",
    "            is_all_non_event = None\n",
    "            all_non_events = []\n",
    "            start_mark = None\n",
    "\n",
    "            for i, row in enumerate(reader):\n",
    "                if row:\n",
    "                    first_column = row[0]\n",
    "                    next_column = row[1]\n",
    "\n",
    "                    if first_column != '===':\n",
    "\n",
    "                        if next_column == 'O':\n",
    "                            fst_col_split = first_column.split(' | ')\n",
    "\n",
    "                            # The line was required to be annotated annotation\n",
    "                            if len(fst_col_split) > 1:\n",
    "                                # The line was annotated, annotation is set on the next line\n",
    "                                if not fst_col_split[1]:\n",
    "                                    mark_row = reader.__next__()\n",
    "                                    is_event = mark_row[1] == 'IS_EVENT'\n",
    "                                    if not start_mark:\n",
    "                                        start_mark = mark_row[0]\n",
    "                                    if is_all_non_event is not None:\n",
    "                                        is_all_non_event = not is_event\n",
    "                                    line = (fst_col_split[0], is_event)\n",
    "                                    if not len(miltiline_events_cache):\n",
    "                                        lines.append(line)\n",
    "                                        if is_all_non_event:\n",
    "                                            all_non_events.append(line)\n",
    "                                    else:\n",
    "                                        miltiline_events_cache.append(line)\n",
    "                                        is_all_non_event = False\n",
    "                                # Not annotated\n",
    "                                else:\n",
    "                                    if is_all_non_event:\n",
    "                                        all_non_events.append(\n",
    "                                            (fst_col_split[0], False))\n",
    "                                    mark = re.findall(\n",
    "                                        'REL_\\\\d+', fst_col_split[1])\n",
    "                                    if mark:\n",
    "                                        start_mark = mark[0]\n",
    "                                if len(miltiline_events_cache):\n",
    "                                    # We already have some sentences in the cache\n",
    "                                    # Updating their status\n",
    "                                    ee = [miltiline_events_cache[i:i + 2]\n",
    "                                          for i in range(0, len(miltiline_events_cache), 2)]\n",
    "                                    for e in ee:\n",
    "                                        r = []\n",
    "                                        for x in e:\n",
    "                                            a, b = x\n",
    "                                            if a not in r:\n",
    "                                                r.append(a.strip())\n",
    "                                        t = ' '.join(r)\n",
    "                                        lines.append((t, is_event))\n",
    "                                    miltiline_events_cache = []\n",
    "                            else:\n",
    "                                event_text = fst_col_split[0].strip()\n",
    "                                # Append multi-event text if we already have something in the cache\n",
    "                                if event_text and len(miltiline_events_cache):\n",
    "                                    miltiline_events_cache.append(\n",
    "                                        (event_text, next_column))\n",
    "                                    is_all_non_event = False\n",
    "                        elif not (row[0].startswith('REL_') or row == 'END'):\n",
    "                            line = (row[0], next_column)\n",
    "                            miltiline_events_cache.append(line)\n",
    "                            is_all_non_event = False\n",
    "                    # We reached the end\n",
    "                    if first_column == start_mark:\n",
    "                        if is_all_non_event and next_column == 'IS_NOT_EVENT':\n",
    "                            lines += all_non_events\n",
    "                        all_non_events = []\n",
    "                count += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_docs(annotated_texts):\n",
    "    docs = []\n",
    "    for i, (text, is_event) in enumerate(annotated_texts):\n",
    "        try:\n",
    "            doc = nlp(text)\n",
    "            docs.append((doc, is_event))\n",
    "        except Exception as e:\n",
    "            print(f'Failed to create nlp from text that starts with: {text[:50]} {e}')\n",
    "        if i % 500 == 0:\n",
    "            print('-->', i)\n",
    "    return docs\n",
    "        \n",
    "\n",
    "def parse_features(feats_string):\n",
    "    res = {}\n",
    "    feats = feats_string.split('|')\n",
    "    for feat in feats:\n",
    "        k, v = feat.split('=')\n",
    "        res[k] = v\n",
    "    return res\n",
    "\n",
    "\n",
    "def find_dates(string, is_future=False):\n",
    "    valid_months = ['січня', 'січ.','лютого','лют.','березня','берез.','квітня','квіт.',\n",
    "                'травня','трав.','червня','черв.','липня','лип.','серпня','серп.',\n",
    "               'вересня','верес.','жовтня','жовт.','листопада','листоп.','грудня','груд.']\n",
    "    mon_regex_str = '|'.join(valid_months).replace('.', '\\.')\n",
    "    regex = '(\\s\\d{4}\\s|(\\d+ (' + mon_regex_str + ')(\\s\\d{4})?)|\\d{2,4}-\\d{2}-\\d{2,4}|\\d{2}.\\d{2}.\\d{2,4}|\\d{2}\\/\\d{2}\\/\\d{2,4})'\n",
    "    matches = re.findall(regex, string, re.IGNORECASE)\n",
    "    dates = []\n",
    "    for match in matches:\n",
    "        date = match[0].strip()\n",
    "        if len(date) == 4:\n",
    "            curr_year = datetime.datetime.now().year\n",
    "            # fragile thing, it may predict date if it's actually some other 4-digit stuff\n",
    "            if int(date) <= curr_year or is_future:\n",
    "                dates.append(date)\n",
    "        else:\n",
    "            dates.append(date)\n",
    "    return dates\n",
    "\n",
    "\n",
    "def get_doc_core_members(doc):\n",
    "    res = []\n",
    "    \n",
    "    adv_final = ['вперше', 'нарешті', 'врешті', 'вчора', 'сьогодні', 'позавчора']\n",
    "    \n",
    "    \n",
    "    def get_token_children(token, tree):\n",
    "        return [x for x in tree if x.head == int(token.id)]\n",
    "    \n",
    "    def get_token_window(token_id, tokens):\n",
    "        lefts, rights = [], []\n",
    "        if token_id > 3:\n",
    "            lefts.append(tokens[token_id - 4])\n",
    "        if token_id > 2:\n",
    "            lefts.append(tokens[token_id - 3])\n",
    "        if token_id > 1:\n",
    "            lefts.append(tokens[token_id - 2])\n",
    "        neigb_right = token_id + 1\n",
    "        while neigb_right < len(tokens) - token_id:\n",
    "            rights.append(tokens[neigb_right])\n",
    "            neigb_right += 1\n",
    "        return lefts, rights\n",
    "    \n",
    "\n",
    "    def get_root_ccomp_verb(root_id, tree):\n",
    "        for word in tree.words:\n",
    "            if word.deprel == 'ccomp' and word.head == root.id:\n",
    "                if word.upos == 'VERB':\n",
    "                    return word\n",
    "                for child in get_token_children(word, tree.words):\n",
    "                    if child.upos == 'VERB':\n",
    "                        return child\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        spo = {}\n",
    "        pred = None\n",
    "        subj = None\n",
    "        obj = None\n",
    "        \n",
    "        num_words = len(sent.words)\n",
    "        \n",
    "        root = next((word\n",
    "                     for word in sent.words if word.deprel == 'root'),\n",
    "                    None)\n",
    "        if not doc.text.strip() or num_words == 2 and sent.words[num_words - 1].upos == 'PUNCT' or root.upos == 'SYM':\n",
    "            continue\n",
    "        # FIXME: iterate only once\n",
    "        if root:\n",
    "            root_conj = next((word for word in sent.words if word.deprel ==\n",
    "                              'conj' and word.head == int(root.id)), None)\n",
    "            root_mod = next((word for word in sent.words if word.deprel ==\n",
    "                              'advmod' and word.upos == 'PART' and word.head == int(root.id)), None)\n",
    "\n",
    "            subj = next((word for word in sent.words if word.deprel ==\n",
    "                         'nsubj' and word.head == int(root.id)), None)\n",
    "            obj = next((word for word in sent.words if word.deprel ==\n",
    "                        'obj' and word.head == int(root_conj.id if root_conj else root.id)),\n",
    "                       None)\n",
    "            c_conj = next((word\n",
    "                     for word in sent.words if word.upos == 'CCONJ' and sent.words[int(word.id) - 2].upos == 'PUNCT'),\n",
    "                    None)\n",
    "            root_adv_final = next((word for word in sent.words if word.deprel ==\n",
    "                        'advmod' and word.upos == 'ADV' and word.head == int(root.id) \\\n",
    "                and word.lemma.lower() in adv_final),\n",
    "                       None)\n",
    "            root_xcomp = next((word for word in sent.words if word.deprel ==\n",
    "                              'xcomp' and word.head == int(root.id)), None)\n",
    "            root_ccomp = get_root_ccomp_verb(int(root.id), sent)\n",
    "            root_xcomp_noun = next((word for word in sent.words if word.deprel == 'xcomp:sp' \\\n",
    "                               and word.upos == 'NOUN' \\\n",
    "                              and word.head == int(root.id)),\n",
    "                                    None)\n",
    "            root_window = get_token_window(int(root.id), sent.words)\n",
    "            \n",
    "\n",
    "            spo['subj'] = subj\n",
    "            spo['root'] = root\n",
    "            spo['root-conj'] = root_conj\n",
    "            spo['obj'] = obj\n",
    "            spo['root_mod'] = root_mod\n",
    "            spo['c_conj'] = c_conj\n",
    "            spo['root_adv_final'] = root_adv_final\n",
    "            spo['root_xcomp'] = root_xcomp\n",
    "            spo['root_ccomp'] = root_ccomp\n",
    "            spo['root_xcomp_noun'] = root_xcomp_noun\n",
    "            spo['root_window'] = root_window\n",
    "            spo['all_verbs'] = [x for x in sent.words if x.upos == 'VERB']\n",
    "            if subj:\n",
    "                subj_conj = next((word for word in sent.words if word.deprel ==\n",
    "                             'conj' and (word.upos == 'NOUN' or word.upos == 'PRON') \\\n",
    "                                  and word.head == int(subj.id)), None)\n",
    "                spo['subj-conj'] = subj_conj\n",
    "                if subj_conj:\n",
    "                    subj_conj_verb = next((word for word in sent.words if word.upos ==\n",
    "                        'VERB' and word.head == int(subj_conj.id)),\n",
    "                       None)\n",
    "                    spo['subj-conj-verb'] = subj_conj_verb\n",
    "                \n",
    "                subj_window = get_token_window(int(subj.id), sent.words)\n",
    "                spo['subj_window'] = subj_window\n",
    "            if obj:\n",
    "                obj_window = get_token_window(int(obj.id), sent.words)\n",
    "                spo['obj_window'] = obj_window\n",
    "\n",
    "        res.append((sent.text, spo, num_words))\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def get_features(doc):\n",
    "    features = []\n",
    "    \n",
    "    predicate_special = ['допустити', 'думати', 'припустити', 'відреагувати', 'пояснити',\n",
    "                     'сказати', 'заявити', 'повідомити', 'повідомляти', 'розповісти',\n",
    "                      'розповідати', 'рекомендувати', 'порекомендувати', 'мати', 'стати', 'почати',\n",
    "                        'назвати']\n",
    "    \n",
    "    def _get_verbs_past(verbs):\n",
    "        past = 0\n",
    "        for verb in verbs:\n",
    "            feats = parse_features(verb.feats)\n",
    "            if feats.get('Tense') == 'Past':\n",
    "                past += 1\n",
    "        return past\n",
    "\n",
    "\n",
    "    spos = get_doc_core_members(doc)\n",
    "    for sent_text, spo, num_words in spos:\n",
    "        feat = {}\n",
    "        if spo:\n",
    "            root = spo['root']\n",
    "            root_conj = spo.get('root-conj')\n",
    "            root_adv_final = spo.get('root_adv_final')\n",
    "            root_xcomp = spo.get('root_xcomp')\n",
    "            root_ccomp = spo.get('root_ccomp')\n",
    "            root_xcomp_noun = spo.get('root_xcomp_noun')\n",
    "            root_lefts, root_rights = spo['root_window']\n",
    "            subj = spo.get('subj')\n",
    "            subj_conj = spo.get('subj-conj')\n",
    "            subj_window = spo.get('subj_window')\n",
    "            obj_window = spo.get('obj_window')\n",
    "            obj = spo.get('obj')\n",
    "            all_verbs = spo['all_verbs']\n",
    "\n",
    "            dates = find_dates(doc.text, True)\n",
    "\n",
    "            if root.feats:\n",
    "                pred_features = parse_features(root.feats)\n",
    "            else:\n",
    "                pred_features = {}\n",
    "\n",
    "            pos_shape = root.upos\n",
    "            if subj:\n",
    "                pos_shape += f'_{subj.upos}'\n",
    "            if obj:\n",
    "                pos_shape += f'_{obj.upos}'\n",
    "            \n",
    "            feat['pos-shape'] = pos_shape\n",
    "            feat['subj'] = 'SUBJ' if subj else 'NONE'\n",
    "            feat['has-date'] = len(dates) > 0\n",
    "            \n",
    "            if len(root_lefts) - 1 > 0:\n",
    "                feat['root-w-1'] = root_lefts[len(root_lefts) - 1].upos\n",
    "            if len(root_lefts) - 2 > 0:\n",
    "                feat['root-w-2'] = root_lefts[len(root_lefts) - 2].upos\n",
    "            if len(root_lefts) - 3 > 0:\n",
    "                feat['root-w-3'] = root_lefts[len(root_lefts) - 3].upos\n",
    "            if len(root_rights) - 1 > 0:\n",
    "                feat['root-w+1'] = root_rights[len(root_rights) - 1].upos\n",
    "            if len(root_rights) - 2 > 0:\n",
    "                feat['root-w+2'] = root_rights[len(root_rights) - 2].upos\n",
    "            if len(root_rights) - 3 > 0:\n",
    "                feat['root-w+3'] = root_rights[len(root_rights) - 3].upos\n",
    "                \n",
    "            if subj_window:\n",
    "                subj_lefts, subj_rights = subj_window\n",
    "                \n",
    "                if len(subj_lefts) - 1 > 0:\n",
    "                    feat['subj-w-1'] = subj_lefts[len(subj_lefts) - 1].upos\n",
    "                if len(subj_lefts) - 2 > 0:\n",
    "                    feat['subj-w-2'] = subj_lefts[len(subj_lefts) - 2].upos\n",
    "                if len(subj_lefts) - 3 > 0:\n",
    "                    feat['subj-w-3'] = subj_lefts[len(subj_lefts) - 3].upos\n",
    "                if len(subj_rights) - 1 > 0:\n",
    "                    feat['subj-w+1'] = subj_rights[len(subj_rights) - 1].upos\n",
    "                if len(subj_rights) - 2 > 0:\n",
    "                    feat['subj-w+2'] = subj_rights[len(subj_rights) - 2].upos\n",
    "                if len(subj_rights) - 3 > 0:\n",
    "                    feat['subj-w+3'] = subj_rights[len(subj_rights) - 3].upos\n",
    "                    \n",
    "            if obj_window:\n",
    "                obj_lefts, obj_rights = obj_window\n",
    "                \n",
    "                if len(obj_lefts) - 1 > 0:\n",
    "                    feat['obj-w-1'] = obj_lefts[len(obj_lefts) - 1].upos\n",
    "                if len(obj_lefts) - 2 > 0:\n",
    "                    feat['obj-w-2'] = obj_lefts[len(obj_lefts) - 2].upos\n",
    "                if len(obj_lefts) - 3 > 0:\n",
    "                    feat['obj-w-3'] = obj_lefts[len(obj_lefts) - 3].upos\n",
    "                if len(obj_rights) - 1 > 0:\n",
    "                    feat['obj-w+1'] = obj_rights[len(obj_rights) - 1].upos\n",
    "                if len(obj_rights) - 2 > 0:\n",
    "                    feat['obj-w+2'] = obj_rights[len(obj_rights) - 2].upos\n",
    "                if len(obj_rights) - 3 > 0:\n",
    "                    feat['obj-w+3'] = obj_rights[len(obj_rights) - 3].upos\n",
    "            \n",
    "                \n",
    "            feat['is_question'] = sent_text.endswith('?')\n",
    "\n",
    "            if pred_features.get('Tense') == 'Past':\n",
    "                feat['root_xcomp'] = root_xcomp is not None\n",
    "                if root_xcomp:\n",
    "                    feat['root_xcomp_pos'] = root_xcomp.upos\n",
    "                if root_ccomp:\n",
    "                    root_ccomp_features = parse_features(root_ccomp.feats)\n",
    "                    feat['root_ccomp_tense'] = root_ccomp_features.get('Tense') or 'NONE'\n",
    "                    feat['root_ccomp_aspect'] = root_ccomp_features.get('Aspect') or 'NONE'\n",
    "                    if root_ccomp_features.get('Tense') != 'Past':\n",
    "                        feat['pred-special'] = root.lemma.lower() in predicate_special\n",
    "            if root_conj:\n",
    "                feat['root_conj_special'] = root_conj.lemma.lower() in predicate_special\n",
    "\n",
    "            feat['all_verb_past'] = _get_verbs_past(all_verbs) == len(all_verbs)\n",
    "\n",
    "            if subj:\n",
    "                subj_features = parse_features(subj.feats)\n",
    "                feat['subj-animacy'] = subj_features.get('Animacy') or 'NONE'\n",
    "                feat['subj-pos'] = subj.upos\n",
    "            else:\n",
    "                feat['subj-animacy'] = 'NONE'\n",
    "                feat['subj-pos'] = 'NONE'\n",
    "\n",
    "            feat['obj'] = 'OBJ' if obj else 'NONE'\n",
    "\n",
    "            if root.upos == 'VERB':\n",
    "                feat['pred-tense'] = pred_features.get('Tense') or 'NONE'\n",
    "                feat['pred-aspect'] = pred_features.get('Aspect') or 'NONE'\n",
    "            if root.upos == 'NOUN' or root.upos == 'PROPN':\n",
    "                feat['pred-anim'] = pred_features.get('Animacy') or 'NONE'\n",
    "                feat['pred-abbr'] = pred_features.get('Abbr') or 'NONE'\n",
    "            \n",
    "            features.append(feat)\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def get_data(docs):\n",
    "    def _get_spo_shape(s, p, o):\n",
    "        ids = [p.id]\n",
    "        if s:\n",
    "            ids.append(s.id)\n",
    "        if o:\n",
    "            ids.append(o.id)\n",
    "        indexes = [str(y) for x, y in sorted([(x, i) for i, x in enumerate(ids)])]\n",
    "        \n",
    "        return '_'.join(indexes)\n",
    "\n",
    "    features, labels = [], []\n",
    "\n",
    "    for doc, is_event in docs:\n",
    "        feats = get_features(doc)\n",
    "        for feat in feats:\n",
    "            features.append(feat)\n",
    "            labels.append(is_event if feat else False)\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_articles_path = '../../data/articles/source_normalized'\n",
    "# annotation_source_path = '../../data/articles/for_annotation'\n",
    "# make_annotattion_sources(source_articles_path, annotation_source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: 3189\n",
      "Is event: 1767\n",
      "Is not event: 1422\n"
     ]
    }
   ],
   "source": [
    "annotated_articles_path = '../../data/articles/annotated'\n",
    "annotated_parsed = parse_annotated_articles(annotated_articles_path)\n",
    "annotated_parsed = list(set(annotated_parsed))\n",
    "annotated_parsed = [(trim_indirect_speech(x), y) for x, y in annotated_parsed]\n",
    "truish = [(x, y) for x, y in annotated_parsed if y]\n",
    "falsish = [(x, y) for x, y in annotated_parsed if not y]\n",
    "print('All:', len(annotated_parsed))\n",
    "print('Is event:', len(truish))\n",
    "print('Is not event:', len(falsish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../data/articles/annotated_parsed.json', 'w') as f:\n",
    "#     json.dump(annotated_parsed, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> 0\n",
      "--> 500\n",
      "--> 1000\n",
      "--> 1500\n",
      "--> 2000\n",
      "--> 2500\n",
      "Failed to create nlp from text that starts with: Батькам нікуди дітей дівати. Кернес не ввів карант \n",
      "--> 3000\n"
     ]
    }
   ],
   "source": [
    "all_docs = get_all_docs(annotated_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y = get_data(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                                    random_state=42, shuffle = True, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('dict_vect',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=True)),\n",
       "                ('lrc',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=800,\n",
       "                                    multi_class='multinomial', n_jobs=-1,\n",
       "                                    penalty='l2', random_state=42, solver='sag',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.72      0.68      0.70       458\n",
      "        True       0.75      0.79      0.77       561\n",
      "\n",
      "    accuracy                           0.74      1019\n",
      "   macro avg       0.74      0.73      0.73      1019\n",
      "weighted avg       0.74      0.74      0.74      1019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_test, clf.predict(data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=True\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.981\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        obj-w+1=PRON\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 82.40%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.817\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=ADJ_PROPN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.03%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.588\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        obj-w+2=DET\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.25%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.574\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        root-w+1=X\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.86%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.538\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        obj-w-2=NUM\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.08%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.525\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        subj-w+1=X\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.08%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.525\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        subj-w-1=PROPN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.28%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.514\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pred-tense=NONE\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.33%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.511\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        obj-w+3=X\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.87%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.480\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=VERB_ADJ_X\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.92%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.477\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        subj-w+2=PRON\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.93%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.477\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        subj-w+2=PROPN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.32%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.455\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=NOUN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.34%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.454\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        root-w+2=X\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.34%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 121 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.25%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 142 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.25%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.459\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        subj-w+1=SCONJ\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.95%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.476\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        subj-w-1=PRON\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.85%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.481\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=PROPN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.72%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.489\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        obj-w-2=AUX\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.51%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.500\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        root-w+2=PRON\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.33%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.511\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pos-shape=ADV_NOUN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.530\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        obj-w-1=X\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.58%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.555\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        root-w+3=ADV\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.14%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.642\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        root-w-1=PART\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.07%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.646\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        root-w-2=PRON\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.71%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.669\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        root-w-1=PRON\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 82.99%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.779\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        obj-w+1=PROPN\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 82.88%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.786\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        root-w+1=PRON\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 82.85%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.787\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        subj-w-2=DET\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 82.65%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.801\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        root-w+3=AUX\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 82.63%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.802\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pred-tense=Fut\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "Explanation(estimator=\"LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=800,\\n                   multi_class='multinomial', n_jobs=-1, penalty='l2',\\n                   random_state=42, solver='sag', tol=0.0001, verbose=0,\\n                   warm_start=False)\", description=\"\\nFeatures with largest coefficients.\\nCaveats:\\n1. Be careful with features which are not\\n   independent - weights don't show their importance.\\n2. If scale of input features is different then scale of coefficients\\n   will also be different, making direct comparison between coefficient values\\n   incorrect.\\n3. Depending on regularization, rare features sometimes may have high\\n   coefficients; this doesn't mean they contribute much to the\\n   classification result for most examples.\\n\", error=None, method='linear model', is_regression=False, targets=[TargetExplanation(target=True, feature_weights=FeatureWeights(pos=[FeatureWeight(feature='obj-w+1=PRON', weight=0.9809912299391859, std=None, value=None), FeatureWeight(feature='pos-shape=ADJ_PROPN', weight=0.8173681802521351, std=None, value=None), FeatureWeight(feature='obj-w+2=DET', weight=0.5877462101343613, std=None, value=None), FeatureWeight(feature='root-w+1=X', weight=0.5740912210730458, std=None, value=None), FeatureWeight(feature='obj-w-2=NUM', weight=0.5380688458122413, std=None, value=None), FeatureWeight(feature='subj-w+1=X', weight=0.5254879154702675, std=None, value=None), FeatureWeight(feature='subj-w-1=PROPN', weight=0.5253370528374552, std=None, value=None), FeatureWeight(feature='pred-tense=NONE', weight=0.5140534748473841, std=None, value=None), FeatureWeight(feature='obj-w+3=X', weight=0.5109962522997041, std=None, value=None), FeatureWeight(feature='pos-shape=VERB_ADJ_X', weight=0.48030784681019156, std=None, value=None), FeatureWeight(feature='subj-w+2=PRON', weight=0.47732068532310234, std=None, value=None), FeatureWeight(feature='subj-w+2=PROPN', weight=0.4769300895723978, std=None, value=None), FeatureWeight(feature='pos-shape=NOUN', weight=0.45481949045126924, std=None, value=None), FeatureWeight(feature='root-w+2=X', weight=0.4538680590942567, std=None, value=None)], neg=[FeatureWeight(feature='pred-tense=Fut', weight=-0.8023382389116731, std=None, value=None), FeatureWeight(feature='root-w+3=AUX', weight=-0.8006086751508803, std=None, value=None), FeatureWeight(feature='subj-w-2=DET', weight=-0.7874063160344831, std=None, value=None), FeatureWeight(feature='root-w+1=PRON', weight=-0.7858590437462536, std=None, value=None), FeatureWeight(feature='obj-w+1=PROPN', weight=-0.7785954792735142, std=None, value=None), FeatureWeight(feature='root-w-1=PRON', weight=-0.6685305608000955, std=None, value=None), FeatureWeight(feature='root-w-2=PRON', weight=-0.6460790623593067, std=None, value=None), FeatureWeight(feature='root-w-1=PART', weight=-0.6416973311016734, std=None, value=None), FeatureWeight(feature='root-w+3=ADV', weight=-0.5547746688165555, std=None, value=None), FeatureWeight(feature='obj-w-1=X', weight=-0.5303287904842672, std=None, value=None), FeatureWeight(feature='pos-shape=ADV_NOUN', weight=-0.5110923922542134, std=None, value=None), FeatureWeight(feature='root-w+2=PRON', weight=-0.5004420393860195, std=None, value=None), FeatureWeight(feature='obj-w-2=AUX', weight=-0.48888089329215517, std=None, value=None), FeatureWeight(feature='pos-shape=PROPN', weight=-0.48105837083082587, std=None, value=None), FeatureWeight(feature='subj-w-1=PRON', weight=-0.47569479278462534, std=None, value=None), FeatureWeight(feature='subj-w+1=SCONJ', weight=-0.4586040291345704, std=None, value=None)], pos_remaining=121, neg_remaining=142), proba=None, score=None, weighted_spans=None, heatmap=None)], feature_importances=None, decision_tree=None, highlight_spaces=None, transition_features=None, image=None)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfr = clf.get_params()['lrc']\n",
    "vec = clf.get_params()['dict_vect']\n",
    "feature_names = get_feature_names(clfr, vec)\n",
    "\n",
    "eli5.explain_weights(clfr, top=30, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./classifier.joblib']"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf, './classifier.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def is_contain_string(string, sub):\n",
    "    overlap_len = int(len(string) * 0.8)\n",
    "    if len(sub) < overlap_len:\n",
    "        return False\n",
    "    return string.find(sub) == 0\n",
    "\n",
    "\n",
    "def is_term_in_title(title_doc, term_doc):\n",
    "    for sent in title_doc.sentences:\n",
    "        for word in sent.words:\n",
    "            for term_word in term_doc.sentences[0].words:\n",
    "                term = term_word.lemma.lower()\n",
    "                w = word.lemma.lower()\n",
    "                is_in = term == w or is_contain_string(w, term) or is_contain_string(term, w)\n",
    "                if is_in:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_root_in_past(doc):\n",
    "    past = 0\n",
    "    for sent in doc.sentences:\n",
    "        root = next((word for word in sent.words if word.deprel == 'root'), None)\n",
    "        root_feats = parse_features(root.feats)\n",
    "        if root_feats.get('Tense') == 'Past':\n",
    "            past += 1\n",
    "    return past == len(doc.sentences)\n",
    "\n",
    "\n",
    "def predict_is_event(title, snippet, term, clf):\n",
    "    title = trim_indirect_speech(title)\n",
    "    if snippet:\n",
    "        snippet = trim_indirect_speech(snippet)\n",
    "    title_lang = langdetect.detect(title)\n",
    "    snippet_lang = langdetect.detect(snippet) if snippet else title_lang\n",
    "\n",
    "    if title_lang != 'uk' and snippet_lang != 'uk':\n",
    "        return False\n",
    "    title_doc = nlp(title)\n",
    "    is_root_past = is_root_in_past(title_doc)\n",
    "    if snippet:\n",
    "        snippet_doc = nlp(snippet)\n",
    "        if is_root_past:\n",
    "            is_root_past = is_root_in_past(title_doc)\n",
    "    if not is_root_past:\n",
    "        return False\n",
    "    term_doc = nlp(term)\n",
    "\n",
    "    is_term_in = is_term_in_title(title_doc, term_doc)\n",
    "    if snippet:\n",
    "        is_term_in = is_term_in or is_term_in_title(snippet_doc, term_doc)\n",
    "    if not is_term_in:\n",
    "        return False\n",
    "    title_features = get_features(title_doc)\n",
    "    is_title_ev = clf.predict(title_features)[0]\n",
    "    if snippet:\n",
    "        snippet_features = get_features(snippet_doc)\n",
    "        is_snippet_ev = clf.predict(snippet_features)[0]\n",
    "\n",
    "    is_ev = is_title_ev\n",
    "\n",
    "    if snippet:\n",
    "        is_ev = is_title_ev or is_snippet_ev\n",
    "\n",
    "    return is_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
