{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('uk', processors='tokenize,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/articles/articles.json') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_to_annotate(src_dir, src_file_name, map_file_name):\n",
    "    src_file_path = os.path.join(src_dir, src_file_name)\n",
    "    with open(src_file_path) as f:\n",
    "        arts = json.load(f)\n",
    "        maps = []\n",
    "        for i, art in enumerate(arts):\n",
    "            content_to_ann = art['title'] + '\\n' + art['content']\n",
    "            ann_dir_path = src_dir + '_ann'\n",
    "            if not os.path.isdir(ann_dir_path):\n",
    "                os.mkdir(ann_dir_path)\n",
    "            filename = os.path.splitext(src_file_name)[0]\n",
    "            with open(os.path.join(ann_dir_path, filename + f'_{i}_.txt'), 'w', encoding='utf-8') as fa:\n",
    "                fa.write(content_to_ann)\n",
    "            maps.append({i: art['url']})\n",
    "        with open(os.path.join(ann_dir_path, map_file_name), 'w', encoding='utf-8') as fm:\n",
    "            json.dump(maps, fm)\n",
    "\n",
    "\n",
    "def get_subj_pred_obj_text_1(text):\n",
    "    res = []\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sentences:\n",
    "            pred = None\n",
    "            subj = None\n",
    "            obj = None\n",
    "            pred = next(((word.id, word.lemma) for word in sent.words if word.deprel == 'root' and word.upos == 'VERB'), None)\n",
    "            if pred:\n",
    "\n",
    "                subj = next(((word.id, word.lemma) for word in sent.words if word.deprel == 'nsubj' and word.head == int(pred[0])), None)\n",
    "                obj = next(((word.id, word.lemma) for word in sent.words if word.deprel == 'obj' and word.head == int(pred[0])), None)\n",
    "            if pred:\n",
    "                res.append({sent.text: (subj[1] if subj else None, pred[1], obj[1] if obj else None)})\n",
    "            else:\n",
    "                res.append({sent.text: None})\n",
    "    except:\n",
    "        print('Failed to create nlp from text that starts with: ' + text[:50])\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_subj_pred_obj_1(corpus, res_file):\n",
    "    current_content = []\n",
    "    with open(res_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(current_content, f)\n",
    "\n",
    "    for i, art in enumerate(corpus):\n",
    "        title_with_spo = get_subj_pred_obj_text_1(art['title'])\n",
    "        content_with_spo = get_subj_pred_obj_text_1(art['content'])\n",
    "        res = {\n",
    "            'url': art['url'],\n",
    "            'date': art['date'],\n",
    "            'title': title_with_spo,\n",
    "            'content': content_with_spo\n",
    "        }\n",
    "        with open(res_file, 'r', encoding='utf-8') as f:\n",
    "            current_content = json.load(f)\n",
    "        current_content.append(res)\n",
    "        \n",
    "        with open(res_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(current_content, f, ensure_ascii=False)\n",
    "        print('>>>', i)\n",
    "    return current_content\n",
    "        \n",
    "\n",
    "def get_train_data(corpus, filename):\n",
    "    data = corpus[:]\n",
    "    return get_subj_pred_obj_1(data, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_with_subj_pred_obj = get_subj_pred_obj_1(corpus[401:], '../../data/articles/test/corona_spo_it_1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/articles/articles_with_spo_it1.json') as f:\n",
    "    dev_arts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = [x['url'] for x in dev_arts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona = 'коронавірус' # TODO: add covid, sars-cov-2, пандем\n",
    "covid = 'covid'\n",
    "sars_cov_2 = 'sars-cov-2'\n",
    "pandem = 'пандем'\n",
    "zelen = 'зеленськ'\n",
    "pres_u = 'президент україни'\n",
    "story = 'сюжет'\n",
    "\n",
    "test_arts_corona = [x for x in corpus if (corona in x['title'].lower() or corona in x['content'].lower() \\\n",
    "                                         or covid in x['title'].lower() or covid in x['content'].lower() \\\n",
    "                                         or sars_cov_2 in x['title'].lower() or sars_cov_2 in x['content'].lower() \\\n",
    "                                         or pandem in x['title'].lower() or pandem in x['content'].lower()) \\\n",
    "                    and story not in x['title'].lower()]\n",
    "# test_arts_zelen_all = [x for x in corpus if (zelen in x['title'].lower() or zelen in x['content'].lower() \\\n",
    "#                                          or pres_u in x['title'].lower() or pres_u in x['content'].lower())]\n",
    "# test_arts_zelen = [x for x in test_arts_zelen_all if story not in x['title'].lower()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_arts_for_mark(arts):\n",
    "    for art in arts:\n",
    "        art.update({'relevant': None})\n",
    "    return arts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(test_arts_corona))\n",
    "print(len(test_arts_zelen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arts_corona = get_test_arts_for_mark(test_arts_corona)\n",
    "# test_arts_zelen = get_test_arts_for_mark(test_arts_zelen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/articles/test/corona.json', 'w') as f:\n",
    "    json.dump(test_arts_corona, f, ensure_ascii=False)\n",
    "with open('../../data/articles/test/zelen.json', 'w') as f:\n",
    "    json.dump(test_arts_zelen, f, ensure_ascii=False)\n",
    "with open('../../data/articles/test/benya.json', 'w') as f:\n",
    "    json.dump(test_arts_benya, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/articles/test/zelen.json') as f:\n",
    "    content = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_urls = [x['url'] for x in content]\n",
    "for art in test_arts_corona[200:250]:\n",
    "    if art['url'] not in ex_urls:\n",
    "        content.append(art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truish = [x for x in content if x['relevant']]\n",
    "print(len(truish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/articles/test/corona.json', 'w') as f:\n",
    "    json.dump(content, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/articles/test/zelen.json') as f:\n",
    "    content = json.load(f)\n",
    "train_data = get_train_data(content, '../../data/articles/test/zelen_spo_it_1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/articles/test/zelen_spo_it_1.json') as zf:\n",
    "    z_content = json.load(zf)\n",
    "with open('../../data/articles/test/corona_spo_it_1.json') as cf:\n",
    "    c_content = json.load(cf)\n",
    "with open('../../data/articles/test/spo_it_1.json', 'w') as f:\n",
    "    res = z_content + c_content\n",
    "    json.dump(res, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/articles/test/corona.json') as f:\n",
    "    expected = [{x['url']: x['relevant']} for x in json.load(f)]\n",
    "    with open('../../data/articles/test/corona_expected.json', 'w') as f:\n",
    "        json.dump(expected, f)\n",
    "with open('../../data/articles/test/zelen.json') as f:\n",
    "    expected = [{x['url']: x['relevant']} for x in json.load(f)]\n",
    "    with open('../../data/articles/test/zelen_expected.json', 'w') as f:\n",
    "        json.dump(expected, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spo(search_obj):\n",
    "    raw_text, spo = [(k, search_obj[k]) for k in search_obj][0]\n",
    "    subj, pred, obj = [x.lower() if x else x for x in (spo if spo else [None, None, None])]\n",
    "    return raw_text, subj, pred, obj\n",
    "\n",
    "\n",
    "def get_is_match(token, spo):\n",
    "    return token.lemma in spo\n",
    "    \n",
    "    \n",
    "def search_by_token(token, article):\n",
    "    _, t_subj, _, t_obj = get_spo(article['title'][0])\n",
    "    is_found = get_is_match(token, [t_subj, t_obj])\n",
    "    if not is_found:\n",
    "        for sent in article['content']:\n",
    "            _, s_subj, _, s_obj = get_spo(sent)\n",
    "            \n",
    "            is_found = get_is_match(token, [s_subj, s_obj])\n",
    "            if is_found:\n",
    "                break\n",
    "    return is_found\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def search_relevant_articles(search_term, corpus):\n",
    "    res = []\n",
    "    search_tokens = nlp(search_term).sentences[0].words\n",
    "\n",
    "    for article in corpus:\n",
    "        is_found = None\n",
    "        title_obj = article['title'][0]\n",
    "        title, t_subj, _, t_obj = get_spo(article['title'][0])\n",
    "        \n",
    "        if len(search_tokens) == 1:\n",
    "            is_found = search_by_token(search_tokens[0], article)\n",
    "        else:\n",
    "            for token in search_tokens:\n",
    "                is_found = search_by_token(token, article)\n",
    "        if is_found:\n",
    "            res.append({'url': article['url'], 'date': article['date'], 'title': title})\n",
    "    return res\n",
    "           \n",
    "\n",
    "\n",
    "def validate_result(result, test_data):\n",
    "    test_true = [x['url'] for x in test_data if x['relevant']]\n",
    "    actual_urls = [x['url'] for x in result]\n",
    "    \n",
    "    true_positives = len([x for x in actual_urls if x in test_true])\n",
    "    false_positives = len([x for x in actual_urls if x not in test_true])\n",
    "    false_negatives = len([x for x in test_true if x not in actual_urls])\n",
    "\n",
    "    act_len = len(actual_urls)\n",
    "\n",
    "    recall = round(true_positives/(true_positives + false_negatives), 2)\n",
    "    precision = round(true_positives/(true_positives + false_positives), 2)\n",
    "\n",
    "    return ({'recall': recall, 'precision': precision})             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../data/articles/train/train_spo_it_1.json') as f:\n",
    "#     corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_corona = 'коронавірус'\n",
    "search_zelen = 'зеленський'\n",
    "\n",
    "corona_results = search_relevant_articles(search_corona, corpus)\n",
    "zelen_results = search_relevant_articles(search_zelen, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corona_results))\n",
    "print(len(zelen_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recall': 0.92, 'precision': 0.73}\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/articles/test/zelen.json') as f:\n",
    "    test_data = json.load(f)\n",
    "res = validate_result(zelen_results, test_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recall': 0.1, 'precision': 0.44}\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/articles/test/corona.json') as f:\n",
    "    test_data = json.load(f)\n",
    "res = validate_result(corona_results, test_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30370\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/articles/all_articles.json') as f:\n",
    "    all_arts = json.load(f)\n",
    "    print(len(all_arts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
