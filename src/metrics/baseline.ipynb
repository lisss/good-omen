{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import stanza\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('uk', processors='tokenize,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_to_annotate(src_dir, src_file_name, map_file_name):\n",
    "    src_file_path = os.path.join(src_dir, src_file_name)\n",
    "    with open(src_file_path) as f:\n",
    "        arts = json.load(f)\n",
    "        maps = []\n",
    "        for i, art in enumerate(arts):\n",
    "            content_to_ann = art['title'] + '\\n' + art['content']\n",
    "            ann_dir_path = src_dir + '_ann'\n",
    "            if not os.path.isdir(ann_dir_path):\n",
    "                os.mkdir(ann_dir_path)\n",
    "            filename = os.path.splitext(src_file_name)[0]\n",
    "            with open(os.path.join(ann_dir_path, filename + f'_{i}_.txt'), 'w',\n",
    "                      encoding='utf-8') as fa:\n",
    "                fa.write(content_to_ann)\n",
    "            maps.append({i: art['url']})\n",
    "        with open(os.path.join(ann_dir_path, map_file_name), 'w', encoding='utf-8') as fm:\n",
    "            json.dump(maps, fm)\n",
    "\n",
    "\n",
    "def get_subj_pred_obj_text_1(text):\n",
    "    res = []\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sentences:\n",
    "            pred = None\n",
    "            subj = None\n",
    "            obj = None\n",
    "            pred = next(((word.id, word.lemma)\n",
    "                         for word in sent.words if word.deprel == 'root' and word.upos == 'VERB'),\n",
    "                        None)\n",
    "            if pred:\n",
    "\n",
    "                subj = next(((word.id, word.lemma) for word in sent.words if word.deprel ==\n",
    "                             'nsubj' and word.head == int(pred[0])), None)\n",
    "                obj = next(((word.id, word.lemma) for word in sent.words if word.deprel ==\n",
    "                            'obj' and word.head == int(pred[0])), None)\n",
    "            if pred:\n",
    "                res.append(\n",
    "                    {sent.text: (subj[1] if subj else None, pred[1], obj[1] if obj else None)})\n",
    "            else:\n",
    "                res.append({sent.text: None})\n",
    "    except:\n",
    "        print('Failed to create nlp from text that starts with: ' + text[:50])\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_subj_pred_obj_1(corpus, res_file):\n",
    "    current_content = []\n",
    "    with open(res_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(current_content, f)\n",
    "\n",
    "    for i, art in enumerate(corpus):\n",
    "        title_with_spo = get_subj_pred_obj_text_1(art['title'])\n",
    "        content_with_spo = get_subj_pred_obj_text_1(art['content'])\n",
    "        res = {\n",
    "            'url': art['url'],\n",
    "            'date': art['date'],\n",
    "            'title': title_with_spo,\n",
    "            'content': content_with_spo\n",
    "        }\n",
    "        with open(res_file, 'r', encoding='utf-8') as f:\n",
    "            current_content = json.load(f)\n",
    "        current_content.append(res)\n",
    "\n",
    "        with open(res_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(current_content, f, ensure_ascii=False)\n",
    "        print('>>>', i)\n",
    "    return current_content\n",
    "\n",
    "\n",
    "def get_test_arts_for_mark(arts):\n",
    "    for art in arts:\n",
    "        art.update({'relevant': None})\n",
    "    return arts\n",
    "\n",
    "\n",
    "def get_spo(search_obj):\n",
    "    raw_text, spo = [(k, search_obj[k]) for k in search_obj][0]\n",
    "    subj, pred, obj = [x.lower() if x else x for x in (\n",
    "        spo if spo else [None, None, None])]\n",
    "    return raw_text, subj, pred, obj\n",
    "\n",
    "\n",
    "def get_is_match(token, spo):\n",
    "    return token.lemma in spo\n",
    "\n",
    "\n",
    "def search_by_token(token, article):\n",
    "    _, t_subj, _, t_obj = get_spo(article['title'][0])\n",
    "    is_found = get_is_match(token, [t_subj, t_obj])\n",
    "    if not is_found:\n",
    "        for sent in article['content']:\n",
    "            _, s_subj, _, s_obj = get_spo(sent)\n",
    "\n",
    "            is_found = get_is_match(token, [s_subj, s_obj])\n",
    "            if is_found:\n",
    "                break\n",
    "    return is_found\n",
    "\n",
    "\n",
    "def search_relevant_articles(search_term, corpus):\n",
    "    res = []\n",
    "    search_tokens = nlp(search_term).sentences[0].words\n",
    "\n",
    "    for article in corpus:\n",
    "        is_found = None\n",
    "        title_obj = article['title'][0]\n",
    "        title, t_subj, _, t_obj = get_spo(article['title'][0])\n",
    "\n",
    "        if len(search_tokens) == 1:\n",
    "            is_found = search_by_token(search_tokens[0], article)\n",
    "        else:\n",
    "            for token in search_tokens:\n",
    "                is_found = search_by_token(token, article)\n",
    "        if is_found:\n",
    "            res.append(\n",
    "                {'url': article['url'], 'date': article['date'], 'title': title})\n",
    "    return res\n",
    "\n",
    "\n",
    "def validate_result(result, test_data):\n",
    "    test_true = [x['url'] for x in test_data if x['relevant']]\n",
    "    actual_urls = [x['url'] for x in result]\n",
    "\n",
    "    true_positives = len([x for x in actual_urls if x in test_true])\n",
    "    false_positives = len([x for x in actual_urls if x not in test_true])\n",
    "    false_negatives = len([x for x in test_true if x not in actual_urls])\n",
    "\n",
    "    act_len = len(actual_urls)\n",
    "\n",
    "    recall = round(true_positives/(true_positives + false_negatives), 2)\n",
    "    precision = round(true_positives/(true_positives + false_positives), 2)\n",
    "\n",
    "    return ({'recall': recall, 'precision': precision})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/articles/train/train_spo_it_1.json') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_corona = 'коронавірус'\n",
    "search_zelen = 'зеленський'\n",
    "\n",
    "corona_results = search_relevant_articles(search_corona, corpus)\n",
    "zelen_results = search_relevant_articles(search_zelen, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recall': 0.92, 'precision': 0.73}\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/articles/test/zelen.json') as f:\n",
    "    test_data = json.load(f)\n",
    "res = validate_result(zelen_results, test_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recall': 0.1, 'precision': 0.44}\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/articles/test/corona.json') as f:\n",
    "    test_data = json.load(f)\n",
    "res = validate_result(corona_results, test_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30370\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/articles/all_articles.json') as f:\n",
    "    all_arts = json.load(f)\n",
    "    print(len(all_arts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
